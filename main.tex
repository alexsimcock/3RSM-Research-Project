% Document geometry and pre-requisites
\documentclass[12pt]{article}
\usepackage[a4paper, margin=2cm]{geometry}
% Bibliography / referencing
\usepackage[style = authoryear]{biblatex}
\addbibresource{bibliography.bib}
% Mathematical symbols
\usepackage{amsmath, amssymb, mathtools, mathrsfs}
% Notationally neat derivatives
\usepackage[thinc]{esdiff}
% Algorithms
\usepackage{algorithm,algpseudocode}
% Tikz graphics
\usepackage{graphicx, tikz}
\usetikzlibrary{shapes, arrows, positioning, calc}
% Tikz rectangle node (https://tex.stackexchange.com/questions/47704/how-to-establish-node-anchor-like-points-on-a-tikz-rectangle-path-is-there-a)
\usetikzlibrary{fit}
\makeatletter
\tikzset{
  fitting node/.style={
    inner sep=0pt,
    fill=none,
    draw=none,
    reset transform,
    fit={(\pgf@pathminx,\pgf@pathminy) (\pgf@pathmaxx,\pgf@pathmaxy)}
  },
  reset transform/.code={\pgftransformreset}
}
\makeatother
% Nesting figures
\usepackage{subcaption}
% Separating file into neater sections
\usepackage{subfiles}
% Smarter label referencing
\usepackage{hyperref}
\usepackage{cleveref}

% Custom operators:
\newtheorem{definition}{Definition}[section]\newtheorem{assumption}{Assumption}

\newcommand{\ind}[1]{\mathbf{1}_{\{#1\}}}
\newcommand{\yhat}{\hat{Y}}
\newcommand{\ptheta}{p_\theta}

\newcommand{\noun}{\textbf{N}}
\newcommand{\verbsym}{\textbf{V}}
\newcommand{\adj}{\textbf{J}}
\renewcommand{\det}{\textbf{D}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\countfunc}{count}

%\title{Sequential Labelling Techniques for Natural Language}
%\title{Sequence Labelling with Graphical Models with an Application in Natural Language Processing}
\title{Graphical Sequence Labelling for Natural Language}
\author{Alexander Simcock (2146154)}
\date{}

\begin{document}

%TODO: settle on definitive notation
% Key:
% Tag Sequence Y, element within Y_i
% Tag alphabet \mathcal{Y}
% Word Sequence X, element within X_i

% Tag states are i or j
% Word emissions are k

% Transition matrix A, a_{ij}
% Emission matrix B, b_j(t)
% Start transition \pi, \pi_i
% Parameters \theta

% Training data X^{(n)} n = 1 to N
% Features k=1 to K
% Sequence length t=1 to T

% Notation worth adjusting:
% p(X \mid \theta) -> p_\theta(X)
% sequence length n -> T

\maketitle

\section{Introduction}
\subfile{sections/introduction}

\section{The Na{\"i}ve Bayes Classifier}
\subfile{sections/naive-bayes}

\section{Hidden Markov Models (HMMs)}
\subfile{sections/hidden-markov-models}

\section{Factor Graphs}
\subfile{sections/factor-graphs}

\section{Generative and Discriminative Models}
\subfile{sections/generative-and-discriminative}

\section{Linear-Chain Conditional Random Fields (CRFs)}
\subfile{sections/conditional-random-fields}

\section{Concluding Remarks}

As put by \autocite{andrew-generative-discriminative-2001}. Discriminative models will tend to have greater success when N is large however thgere is a place for gene

\printbibliography

\end{document}