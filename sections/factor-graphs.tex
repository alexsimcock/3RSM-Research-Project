\documentclass[../main.tex]{subfiles}

\begin{document}

Having defined the HMM and its algorithms, it is beneficial to formalise the class of models which it and the na{\"i}ve Bayes belong to.
Doing so will allow for parallels to be made between the na{\"i}ve Bayes, HMM and the succeeding approach, particularly, making the transferability of certain algorithms apparent and succinct.
The methods we have discussed are both amenable to graphical representations, however it is their approach to modelling, specifically factorising, the joint probability $p(\boldY,\boldX) = p(\boldY)p(\boldX \mid \boldY)$ (\cref{eqn:nb-model,eq:hmm-model}) that has mathematically defined their structure and usage.
As a result, the \textbf{factor graph} is an ideal formalism for these approaches.

Factor graphs are bipartite graphs that provide compact representations of the factorisation of a function and are typically applied to probability distributions.
By doing so, they enable efficient computing of certain characteristics of interest, such as marginal distributions.
Formally, the bipartite graph $G = (V,F,E)$ is a factor graph when the sets of nodes $V$ and $F$ respectively index the random variables ($\boldX$ and $\boldY$) and factors (a set of local functions denoted $\{\Psi_a \}_{a=1}^A$).
Semantically, any edge in $E$ between a variable node $x_t \text{ or } y_t \in V$ and factor node $\Psi_a \in F$ indicates that that variable is an argument of $\Psi_a$ in the factorisation of the distribution function.
\begin{definition}%[Factorisation by Factor Graphs]
    Distribution $p(\boldY)$ factorises according to factor graph $G$ if there exists a set of local functions $\{ \Psi_a \}_{a=1}^A$ such that $p(\boldY)$ can be expressed
    \begin{equation*}
        p(\boldY) = \frac{1}{Z} \prod_{a \in F} \Psi_a (\boldY_{N(a)})
    \end{equation*}
    where $N(a)$ is the neighbourhood of node $a$ and $Z$ is a normalising constant defined as
    \begin{equation*}
        Z = \sum_\boldY \prod_{a \in F} \Psi_a (\boldY_{N(a)}).
    \end{equation*}
\end{definition}

For a trivial example, consider the na{\"i}ve Bayes model of the joint probability \cref{eqn:nb-model}, which can be written as a factor graph by defining local functions $\Psi(\boldY) = p(\boldY)$ and $\Psi_t(\boldY,x_t) = p(x_t \mid \boldY)$.
Likewise, the model of the joint probability provided by the HMM \cref{eq:hmm-model} can be factorised according to a factor graph where $Z=1$
\begin{align} \label{eq:hmm-crf-factor-form}
    \Psi_t(j,i,x) &\coloneqq p(y_t=j \mid y_{t-1}=i)p(x_t=x \mid y_t=j), \\
    p(\boldY,\boldX) &= \prod_{t=1}^T \Psi_t(y_t,y_{t-1},x_t).
\end{align}

These models are wholly defined by their joint probability distributions, and so by factorising these distribution by a factor graph we have proven that both models themselves are factor graphs.

\end{document}