\documentclass[../main.tex]{subfiles}

\begin{document}

%It excels at labelling and parsing sequential data due to a learning approach that is able to infer relationships over many rounds of training.
%It is applied often in modern machine learning systems for both natural language processing and computer vision.

The conditional random field (CRF) \autocite{lafferty-crf-2001} is an alternative graphical model that can be understood as a generalisation of the HMM, retaining a similar structure but adopting more flexible assumptions.
This project will be concerned more precisely with a subset of the family of CRFs and the discriminative counterpart to the HMM: the linear-chain CRF.

It can be understood graphically as an undirected HMM which permits any number of connections between the observed sequence $X$ and the nodes of $Y$.
The undirected edges can be seen as no longer indicating processes of transition or emission but much like in the framework of a factor graph as observing local influence between the two nodes.
CRFs are also able to incorporate additional features without concerns of inter-dependencies, thus we may also add new features to $X$ such as suffixes, prefixes and capitalisation which also connect to $Y$.
With these relaxations many new structures become possible, allowing for a much richer context window and more reliable results when estimating $p(Y \mid X)$.
An example of potential linear-chain CRF structure is given in figure \ref{fig:lc-crf-diag}.

\subfile{tikz-figures/lc-crf-diagram}

%Restrictions are placed on the number and kind of connections made by a CRF for the sake of computational/algorithmic complexity and avoiding issues of over-fitting.
%Most commonly this is divided into the \textbf{linear chain CRF} and \textbf{general CRF}.
There exists also the general CRF, which also allows for relationships between non-neighbouring nodes of $Y$.
The general CRF requires entirely new methodologies to compute and so has been considered outside of the scope of this project.

\subsubsection{An Extension of the HMM}
Viewing the lin chain as an extension of the HMM will allow us to simply define the algorithms by adapting the earlier ones...

Notice the similarities to logistic regression, indeed this diagram points out the neat relation of all methods in this paper:

\subfile{tikz-figures/model-relationships}

\begin{definition}[Linear Chain CRF]
    Let $X$, $Y$ be random sequences/vectors variables.
    $\theta_k$ be parameter vector and $\mathcal{F} = \{ f_k (y,y',x) \}_{k=1}^K$ be a set of feature functions.
    The conditional random field is then a probability distribution.
    \begin{equation*}
        p(Y \mid X) = \frac{1}{Z(X)} \prod_{t=1}^{n} \exp \left\{ \sum_{k=1}^K \theta_k f_k (Y_t, Y_{t-1}, X) \right\}
    \end{equation*}
    Where $Z(X)$ is a normalising function over all values of $Y$:
    \begin{equation*}
        Z(X) = \sum_{\forall Y} \prod_{t=1}^{n} \exp \left\{ \sum_{k=1}^K \theta_k f_k (Y_t, Y_{t-1}, X) \right\}
    \end{equation*}
\end{definition}

Can be thought of as a HMM where the transition probability is not fixed as a scaled $p (Y_t = i \mid Y_{t-1} = j)$. We can incorporate features for example from the current observation, orrr alll observations, here actually although we have written $X$, computationally we would consider only those elements of $X$ pertinent to timestep $t$.
i.e. the assumptions are much less restrictive and relationship between $X$ and $Y$ is two-way.

As the number of features increases, computing the probabilties becomes increasingly complex.
$Z(X)$ is also exponentially LARGE, but calculable by forward-backward!

Linear chain CRFs are considered more standard for industrialised use and impose the restriction that elements in the hidden state only interact with their neighbours.


%\subsection{Generalised CRFs}

%\begin{definition}[Generalised CRF]
%    Let $G$ be
%    \begin{equation*}
%        p (Y \mid X) = \frac{1}{Z(X)} \prod_{a=1}^A \Psi_a (Y_a, X_a)
%    \end{equation*}

%\end{definition}

%Differs from general factor graph as $Z$ is now a function of the input. Previously $Z$ may not have been computable but $Z(X)$ may be

%\begin{definition}[Conditional Random Field]
%    Let $G = (V,E)$ be a graph such that $Y$ is indexed by the vertices of $G$: $Y = (Y_v)_{v \in V}$. $(X,Y)$ is a \textbf{conditional random field} if, when conditioned on $X$, the random variables $Y_i$ obey the Markov property with respect to $G$:
%    \begin{equation*}
 %       p(Y_i \mid X, Y_j, i \neq j) = p(Y_i \mid X, Y_j, j \sim i)
  %  \end{equation*}
   % Where $j \sim i$ means $j$ and $i$ are neighbours in $G$.
    %In other words, the likelihood of one tag transitioning to another is reliant exclusively on the present state and no wider memory. IS THIS TRUE? Surely that's a linear chain
%\end{definition}


\subsection{Inference from Linear-Chain CRFs}

Inference in linear chain CRFs is generally parallel to that in HMMs.
Initially it is intractable and exponentially complex to calculate by brute force.
Since we also will need to call this routine often in training, it may be that approximations could be used to even further speed up algorithms - although carry risk of unpredictable effects.
We can, in the linear case, use the same algorithms as in the HMM (generalised models require more in-depth approaches).

\subsubsection{Re-denoting the HMM Inference Algorithms}
First we define the earlier algorithms with new notation.
The HMM:
\begin{equation*}
    p (Y,X) = \prod_t \Psi_t (Y_t, Y_{t-1}, X_t)
\end{equation*}
Where,
\begin{equation*}
    \Psi_t(j,i,x) \coloneqq p (Y_t=j \mid Y_{t-1}=i) p (X_t=x \mid Y_t=j)
\end{equation*}

The forward algorithm caches inner sums... there are $\mathcal{Y}$ such variables
\begin{equation*}
    \alpha_t(j) \coloneqq p (X_{1:t},Y_t=j) = \sum_{\text{all } Y_{1:t-1}} \Psi(j,Y_{t-1},X_t) \prod_{t'=1}^{t-1} \Psi_{t'}(Y_{t'},Y_{t'-1},X_{t'})
\end{equation*}
Which can be calculated by recursion
\begin{gather*}
    \alpha_1(j) = \Psi_1(j,Y_0,X_1) \\
    \alpha_t(j) = \sum_{i \in \mathcal{Y}} \Psi_t(j,i,X_t) \alpha_{t-1}(i)
\end{gather*}
Backwards is reverse...

Again allowing for calculation of $p (Y_{t-1}, Y_t \mid X)$.
For viterbi we swap out summation with maximisation.

The FB algo is now identical! Simply with a new definition of $\Psi_t$.

% WE NEED NOTATION FOR MORE THAN JUST XT (ALL RELEVANT X)
In the CRF
\begin{equation*}
    \Psi_t (Y_t,Y_{t-1},X_t) = \exp \left\{ \sum_k \theta_k f_k(Y_t,Y_{t-1},X_t)\right\}
\end{equation*}

Although the algorithm is the same our interpretation does change - since the model is discriminative and not generative. The $\alpha$ and $\beta$ variables do not directly mean anything.

Then
\begin{equation*}
    p (Y_t=i \mid X) = \frac{1}{Z(X)} \alpha_t(i)\beta_t(i)
\end{equation*}

When working with linear chain CRFs we are able to easily adapt the suite of iterative algorithms that we previously developed for HMMs.
As in that case the calculations still provide exact figures however the key variables $\alpha$, $\beta$, $\gamma$, $\delta$ lose the neat interpretations as themselves being probability distributions - as a result of the switch from a generative to discriminative approach.


General CRFs will require other methods such as monte carlo (unbiased) or variational (which are biased but faster and borrow from the field of optimisation).
Any graph inference techniques can be used.

\subsection{Parameter Estimation}

Log likelihood, we define a likelihood function like so
and can compute it exactly! From earlier...
after this it is wise to apply stochastic gradient methods...
The original paper references \autocite{della-1997-crfparam}

Minimise regularized negative log probability...
Taken from a corpus $\mathcal{D} = \{X^{(s)}, Y^{(s)}\}_{s=1}^S$. Assume each $X$ is length $n$, for simplicity of notation - it does not contradict the derivation. 
\begin{equation*}
    L(\theta) = \sum_{s=1}^S \log p_\theta(Y^{(s)} \mid X^{(s)})
\end{equation*}
We want to maximise
Substitute it into the CRF model (2.18) and also add regularization.
With large sets of parameters - typically reaching into the hundreds of thousands , we use regularization which reduces the chances of overfitting by penalising weight vectors with a norm that is too large. A common choice is the euclidean norm with a regularization parameter $1/(2\sigma^2)$ \autocite{sutton-2012-crfintro}. Where $\sigma^2$ is a free parameter that can be chosen to determine how harshly to penalise %lso be viewed as performing maximum a posteriori (MAP) estimation of θ, if θ is assigned a Gaussian prior with mean 0 and covariance σ2I.
\begin{equation*}
    L(\theta) = \sum_{s=1}^S \sum_{t=1}^n \sum_{k=1}^K \theta_k f_k(Y_t^{(s)}, Y_{t-1}^{(s)}, X_t^{(s)}) - \sum_{s=1}^S \log Z(X^{(s)}) - \sum_{k=1}^K \frac{\theta_k^2}{2\sigma^2}
\end{equation*}
\begin{equation*}
    \diffp{L}{{\theta_k}} = \sum_{s=1}^S  \sum_{t=1}^n f_k(Y_t^{(s)}, Y_{t-1}^{(s)}, X_t^{(s)}) - \sum_{s=1}^S \sum_{t=1}^T \sum_{y,y'} f_k(y,y',X_t^{(s)})p(y,y'\mid X^{(s)}) - \frac{\theta_k}{\sigma^2} 
\end{equation*}

%\subsubsection{Label Bias Problem}
%The CRF was initially motivated by the label bias problem.
%Occurs when the data is not representative of every possible outcome.
%Label states with only one outgoing transitions take little heed of their observation.
%Regardless of how often this observation has been seen all of the probability mass is passed along.
%If one overall word sequences is seen more often in training, this will now be chosen...

\end{document}

- **Conditional random fields (CRFs)** are a probabilistic framework for modeling structured outputs, such as sequences, trees, and graphs, given a set of input features.
- **CRFs** combine the advantages of **discriminative classification** and **graphical modeling**, allowing for rich, overlapping features and compact representation of complex dependencies among output variables¹[1].
- **CRFs** can be seen as a generalization of **logistic regression** and a discriminative analogue of **hidden Markov models (HMMs)**, which are generative models of structured data²[2]³[3].
- **CRFs** can be trained by **maximum likelihood estimation** using gradient-based optimization methods, such as **L-BFGS** or **stochastic gradient descent**. The gradient computation requires **inference** in the model, which can be done exactly for some structures, such as linear chains, or approximately for others, such as general graphs, using methods such as **belief propagation** or **Markov chain Monte Carlo**.
- **CRFs** have been successfully applied to many tasks in **natural language processing**, **computer vision**, and **bioinformatics**, often achieving state-of-the-art results⁴[4]. They can also be extended to include **latent variables**, **higher-order factors**, and **non-linear features**.