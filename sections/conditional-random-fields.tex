\documentclass[../main.tex]{subfiles}

\begin{document}

%It excels at labelling and parsing sequential data due to a learning approach that is able to infer relationships over many rounds of training.
%It is applied often in modern machine learning systems for both natural language processing and computer vision.

The conditional random field (CRF) \autocite{lafferty-crf-2001} is another graphical model that can be understood as a generalisation of the HMM, retaining a similar structure but adopting more flexible assumptions.
This project will be concerned with only a subset of the family of CRFs, which form a generative-discriminative pair with the HMM: the linear-chain CRF.

The linear-chain CRF can be understood graphically as an undirected HMM which permits any number of connections between the observed sequence $\boldX$ and the nodes of $\boldY$.
The edges of a CRF no longer indicate processes of transition or emission, but instead a general dependency between the two nodes.
This relaxations permits many new structures, allowing for a much richer context window and more reliable results when estimating $p(\boldY \mid \boldX)$.
Of particular interest is the resulting ability to incorporate new features without giving rise to concerns of inter-dependencies.
We could include any number of features to create a CRF that is able to leverage an immense amount of contextual information, such features could include suffixes, capitalisation, domain-specific lexicon membership and more. Most crucially when compared to the HMM, we are able to utilise information that extends outside of the current index, for example by defining a feature which checks whether the word is preceded by `the'.
An example of potential linear-chain CRF structure is given in figure \ref{fig:lc-crf-diag}.
\subfile{tikz-figures/lc-crf-diagram}
General CRFs are even less restrictive, allowing for connections to be drawn between non-neighbouring nodes of $\boldY$. However, this addition gives rise to considerably increased complexity even for singular inferences, requiring completely new graphical inference methods.
The general CRF demands an immense amount of data and computing power to implement and so the linear-chain CRF can be seen as a more economical approach.

%TODO: parameters?
To define the linear-chain CRF we first note that the linear-chain CRF itself can be thought of as a HMM with particular feature functions.
Hence, it is sensible to find a generalised but equivalent formulation of the HMM joint probability \cref{eq:hmm-model}, which we can then commute to find the conditional probability.
This process is parallel to the transformation of the na{\"i}ve Bayes to logistic regression.
An appropriate form for such a generalisation is
\begin{equation} \label{eq:hmm-advanced-form}
    p(\boldY,\boldX) = \frac{1}{Z} \prod_{t=1}^T \exp \left\{ \sum_{i,j \in \mathcal{Y}} \theta_{i,j} \ind{y_t=i}\ind{y_{t-1}=j} + \sum_{i \in \mathcal{Y}} \sum_{k \in \mathcal{X}} \mu_{k,i} \ind{y_t = i}\ind{x_t=k} \right\},
\end{equation}
where $\theta = \{\theta_{i,j},\mu_{i,k}\}$ are the parameters of the HMM and $Z$ is a normalisation constant such that the distribution sums to one.
Thankfully this formulation permits more flexibility without adding any new classes of model \autocite{sutton-2012-crfintro}.
The HMM of the kind we have encountered can be summarised in this format by taking
\begin{align*}
    \theta_{i,j} &= \log{p(y_{t-1}=i \mid y_t=j)} = \log{a_{i,j}}, \\
    \mu_{i,k} &= \log{p(x_t=k \mid y_t=i)} = \log{b_i(k)}, \\
    Z&=1
\end{align*}
and declaring feature functions $f_{i,j}(y,y',x) = \ind{y=i}\ind{y'=j}$ and $f_{i,k}(y,y',x)=\ind{y= i}\ind{x=k}$.
Hence, we can simplify \cref{eq:hmm-advanced-form} to
\begin{equation*}
    p(\boldY,\boldX) = \frac{1}{Z} \prod_{t=1}^T \exp \left\{ \sum_{k=1}^K \theta_k f_k(y_t, y_{t-1}, x_t) \right\}.
\end{equation*}
We then apply this formula to derive the distribution of the linear-chain CRF:
\begin{equation*}
    p(\boldY \mid \boldX) = \frac{p(\boldY,\boldX)}{\sum_{\boldY'} p(\boldY',\boldX)} = \frac{\prod_{t=1}^T \exp \left\{ \sum_{k=1}^K \theta_k f_k(y_t, y_{t-1}, x_t) \right\}}{\sum_{\boldY'} \prod_{t=1}^T  \exp \left\{ \sum_{k=1}^K \theta_k f_k(y'_t, y'_{t-1}, x_t) \right\}}.
\end{equation*}
Since this model has arisen from a HMM, it is currently only configured to use the same feature set as the HMM.
This is easily resolved by simply allowing the feature functions be more general.
To do so neatly, we define $\boldX_t$ which is the vector of all features in $\boldX$ that the linear-chain CRF accesses across all of its feature functions at any one index $t$.
\begin{definition}[Linear Chain CRF]
    Let $\boldX$, $\boldY$ be random sequences/vectors variables.
    $\theta_k$ be parameter vector and $\mathcal{F} = \{ f_k (y,y',\boldX_t) \}_{k=1}^K$ be a set of feature functions.
    The linear-chain conditional random field is then a probability distribution.
    \begin{equation} \label{eq:lc-crf-def}
        p(\boldY \mid \boldX) = \frac{1}{Z(\boldX)} \prod_{t=1}^{T} \exp \left\{ \sum_{k=1}^K \theta_k f_k (y_t, y_{t-1}, \boldX_t) \right\}.
    \end{equation}
    Where $Z(\boldX)$ is a normalising function over all values of $\boldY$:
    \begin{equation*}
        Z(\boldX) = \sum_{\boldY} \prod_{t=1}^{T} \exp \left\{ \sum_{k=1}^K \theta_k f_k (y_t, y_{t-1}, \boldX_t) \right\}.
    \end{equation*}
\end{definition}
Note, once more, that the linear-chain CRF can also be shown to be a factor graph by re-writing \cref{eq:lc-crf-def} with the choice of $\Psi_t$
\begin{equation} \label{eq:crf-factor-graph}
    \Psi_t(y_t, y_{t-1}, x_t) = \exp \left\{ \sum_{k=1}^K \theta_k f_k (y_t, y_{t-1}, \boldX_t) \right\}.
\end{equation}
One may note the similarities of this distribution to the logistic regression model \cref{eq:log-reg}. Indeed, as the HMM was a sequential extension of the na{\"i}ve Bayes, the linear-chain CRF is a sequential logistic regression model.
The interrelatedness of the methods in this project is summarised in \cref{fig:nb-and-hmm-diag}.
\subfile{tikz-figures/model-relationships}

\subsection{Inference}

Given the clear similarities between the two models, it should be unsurprising that the suite of iterative algorithms for HMMs derived in \cref{sec:fb-algo,sec:viterbi-algo} can be easily adapted for use on linear-chain CRFs.
Indeed the leaps in computational efficiency these algorithms make are even more crucial given the much larger set of features we would aim to incorporate in a CRF model.
First, we re-define the algorithms for the HMM, but this time using the factor graph notation introduced by \cref{eq:hmm-crf-factor-form} that will more easily transfer to linear-chain CRFs.
\begin{align*}
    \alpha_t(j) &\coloneqq p(\boldX_{1:t},Y_t=j) = \sum_{\boldY_{1:t-1}} \Psi(j,y_{t-1},x_t) \prod_{t'=1}^{t-1} \Psi_{t'}(y_{t'},y_{t'-1},x_{t'}), \\
    \beta_t(j) &\coloneqq p(\boldX_{t+1:T},y_t=j) = \sum_{\boldY_{t+1:T}} \prod_{t'=t+1}^{T} \Psi_{t'}(y_{t'},y_{t'-1},x_{t'}), \\
    \delta_t(j) &\coloneqq  \max_{\boldY_{1:t-1}} \Psi_t(j,y_{t-1},x_t) \prod_{t'=1}^{t-1} \Psi_{t'}(y_{t'},y_{t'-1},x_{t'}).
\end{align*}
Next, the recursive steps are also written in terms of $\Psi_t$,
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \alpha_1(j) = \Psi_1(j,y_0,x_1) \\
        \text{Inductive Step:} & \quad \alpha_t(j) = \sum_{i \in \mathcal{Y}} \Psi_t(j,i,x_t) \alpha_{t-1}(i) \\
        \text{Base Case:} & \quad \beta_T(i) = 1 \\
        \text{Inductive Step:} & \quad \beta_{t-1}(i) = \sum_{j \in \mathcal{Y}} \Psi_{t} (j,i,x_{t})\beta_{t}(j) \\
        \text{Base Case:} & \quad \delta_1(j) = \max_i \Psi_1(j,y_0,x_1) \\
        \text{Inductive Step:} & \quad \delta{t}(j) = \max_i \Psi_t(j,i,x_t) \delta_{t-1}(i)
    \end{aligned} \qquad 
        \begin{array}{lr}
            i,j \in \mathcal{Y} \\
            2 \leq t \leq T
        \end{array}
\end{equation*}
Finally, we reformulate $\xi$ which we will need for parameter estimation:
\begin{align*}
    \begin{aligned}
    \xi_t(i,j) \coloneqq & \ptheta(y_t = i, y_{t+1} = j \mid \boldX) \\
    &= \frac{\alpha_{t}(i)\Psi_t(i,j,x_{t+1})\beta_{t+1}(j)}{\ptheta(\boldX)}
    \end{aligned} \qquad \begin{array}{lr}
            i,j \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{align*}
%pg53
This reformulation has not altered the values of the helper functions in any way and so these new identities can be applied in the forward-backward and Viterbi algorithms in exactly the same manner as before.
Crucially, the factor graph representation presented here allows for these two algorithms to be easily extended to linear-chain CRFs by substituting $\Psi_t$ with its linear-chain CRF form (given in \cref{eq:crf-factor-graph}).
We can therefore perform the forward-backward and Viterbi algorithms on linear-chain CRFs, with the sole side effect that the helper functions are no longer neatly interpretable as probability distributions.
Instead, the forward-backward algorithm will now return the normalising function $Z(\boldX)$ rather than $\ptheta(\boldX)$,
\begin{equation*}
    Z(\boldX) = \sum_{i \in \mathcal{Y}}\alpha_T(i) = \beta_0(y_0).
\end{equation*}
Thankfully, this does not alter the computation of the marginal distributions:
\begin{align*}
    \gamma_t(i) &= \ptheta(y_t \mid \boldX) = \frac{\alpha_t(y_t)\beta_t(y_t)}{Z(\boldX)}, \\
    \xi_t(i,j) &= \ptheta(y_t=i,y_{t+1}=j\mid \boldX) = \frac{\alpha_{t}(i)\Psi_t(i,j,x_{t+1})\beta_{t+1}(j)}{Z(\boldX)}.
\end{align*}

\subsection{Parameter Estimation}

The Baum-Welch algorithm is no longer appropriate when training the linear-chain CRF, indeed we have many more parameters and calculating the expected values for each would be highly cumbersome, if not completely infeasible.
Thankfully, there are several other approaches from the field of nonlinear optimisation which are well-suited to this task.
In fact, the exponential form of the linear-chain CRF has highly desirable properties for optimisation.
We begin with the typical log-likelihood function $L(\theta)$ which we wish to maximise,
\begin{equation*}
    L(\theta) = \sum_{n=1}^N \log p_\theta(\boldY^{(i)} \mid \boldX^{(i)}).
\end{equation*}
We then substitute this function into the linear-chain CRF model \cref{eq:lc-crf-def}:
\begin{equation} \label{eq:lc-crf-log-likeli}
        L(\theta) = \sum_{i=1}^N \sum_{t=1}^T \sum_{k=1}^K \theta_k f_k(y_t^{(i)}, y_{t-1}^{(i)}, x_t^{(i)}) - \sum_{i=1}^N \log Z(\boldX^{(i)}).
\end{equation}
It is then sensible to introduce a regularisation term $\frac{\theta_k^2}{2\sigma^2}$:
\begin{equation*}
        L(\theta) = \sum_{i=1}^N \sum_{t=1}^T \sum_{k=1}^K \theta_k f_k(y_t^{(i)}, y_{t-1}^{(i)}, x_t^{(i)}) - \sum_{i=1}^N \log Z(\boldX^{(i)}) - \sum_{k=1}^K \frac{\theta_k^2}{2\sigma^2}.
\end{equation*}
Much like the earlier example of additive smoothing (\cref{eq:additive-smoothing}) the regulariser will reduce excessively high variance, here by penalising weight vectors with norms that are too large. In this expression $\sigma^2$ is a free parameter that can be chosen to determine the level of penalisation.
Regularisation is particularly important in the context of CRFs as it is not abnormal to encounter models with sets of parameters in the hundreds of thousands \autocite{sutton-2012-crfintro}.
Both the log-likelihood function \cref{eq:lc-crf-log-likeli} and the regularisation term we have chosen to add to it (the euclidean norm) are strictly concave, since both are products of non-negative numbers.\footnote{The log-likelihood function is of the form $\log \sum_i \exp{x_i}$ which is universally convex.}
Since both our initial likelihood function and regulariser are strictly concave, any local solution we are able to converge to will provide globally optimal parameters.
There are several approaches to finding this point of convergence although all will require the derivation of the partial derivative:
\begin{equation*}
    \diffp{L}{{\theta_k}} = \sum_{i=1}^N  \sum_{t=1}^T f_k(y_t^{(i)}, y_{t-1}^{(i)}, x_t^{(i)}) - \sum_{i=1}^N \sum_{t=1}^T \sum_{y,y'} f_k(y,y',x_t^{(i)})p(y,y'\mid \boldX^{(i)}) - \frac{\theta_k}{\sigma^2}. 
\end{equation*}
Gradient ascent is the simplest approach to implement, but is typically considered to converge too slowly. Newton methods are quicker but require the inverting incredibly large matrices as a result of our large parameter space, which is too complex.
Instead, quasi-Newton approaches such as BFGS are most widely implemented and can favourably be used without need for particular specification.

\subsection{Limitations of Linear-Chain CRFs}

Despite the advantageous characteristics of linear-chain CRFs, it cannot be said that implementing such a model is a catch-all solution.
There are several contexts where the generative approach offered by the HMM may indeed be preferable despite its limitations.
Generative models are much less computationally demanding to implement and train and also tend to perform better on smaller datasets \autocite{andrew-generative-discriminative-2001}.
Moreover, were the context for sequence labelling simple enough that a `true' generative model could be built, then its estimates would be exact. The same cannot be said for models trainined discriminatively.

One especially powerful use-case of HMMs applies an adapted Baum-Welch algorithm to train parameters on unsupervised data, that is, data that is not already annotated with POS tags \autocite{kupiec-1992-hmm}.
Analogous methodologies do not exist for discriminative learning, limiting the linear-chain CRFs ability to tag languages with less widely available corpora.

Regardless, when the dataset is large and features are well chosen, CRFs will perform exceedingly well.
Models that build upon the general CRF have been shown to achieve state-of-the-art results of up to 97.24\% accuracy (tested on the Penn Treebank WSJ corpus, a standard industry benchmark) \autocite{tout-pos2003}.

\end{document}