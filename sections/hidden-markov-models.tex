\documentclass[../main.tex]{subfiles}

\begin{document}

As we look to build upon the na{\"i}ve Bayes approach we might first consider reworking the restrictive transition assumption.
Intuitively, an adjective or noun tag should be more likely to succeed the determiner `the' than a verb.
We would expect that as more finely-grained tags are included in the model, many such significant relationships between the tags are likely to emerge.
For this reason we substitute the transition assumption for a Markov assumption.
\begin{assumption}[Markov Assumption] \label{ass:markov-assumption}
    The probability of each tag $y_t$ is dependent (only) on its predecessor $y_{t-1}$. Thus $p(\boldY) = \prod_{t=1}^T p(y_t \mid y_{t-1})$. Under this assumption $\boldY$ is considered a stochastic process that exhibits the Markov property.
\end{assumption}
We also introduce a `start' node $y_0$, which will allow the calculations defined by the Markov assumption to extend to $y_1$.
Adopted alongside the emission assumption, the Markov assumption specifies a \textbf{hidden Markov model (HMM)}.
The framework of the HMM was first described by Leonard E. Baum \autocite{first-hmm} and saw widespread uptake in the field of speech recognition during the 1970's. Since then, HMMs have also seen popular use in fields such as bioinformatics and, indeed, natural language processing.

The HMM can be thought of as a sequential adaptation of the na{\"i}ve Bayes model where the current tag-state $y_t$ probabilistically determines not only the observation $y_t$ but also the succeeding tag $y_{t+1}$.
Much like na{\"i}ve Bayes, the HMM can also be summarised concisely by the factorisation of the joint probability that arises from its assumptions:
\begin{equation} \label{eq:hmm-model} 
    p(\boldY,\boldX) = \prod_{t=1}^T p(y_t \mid y_{t-1})p(x_t \mid y_t).
\end{equation}
The connection between the two models is even more apparent when both are viewed graphically, as in \cref{fig:nb-and-hmm-diag}.
\subfile{tikz-figures/nb-hmm-diagrams}
Both graphs share the node set $\mathcal{Y} \cup \mathcal{X}$, where $\mathcal{X}$ denotes a finite set of all the words that could appear in $\boldX$.
The directed edges of these graphs indicate either transition from one tag to another or the emission of a word from a tag and are weighted according to the probability of the emission/transition they represent.
The graphical interpretation of the HMM therefore assumes that $\boldY$ is generated by a walk on a `hidden' Markov chain over $\mathcal{Y}$ that starts at $y_0$ and ends after $T$ steps.
Likewise, we can understand that the HMM assumes that each word $x_t$ is probabilistically generated by $y_t$, in the same manner as the na{\"i}ve Bayes.

As the graphical representation elucidates, the HMM can be summarised entirely by its transition and emission probabilities, which we store in the matrices $A$, $B$ and $\pi$ (where the vector $\pi$ is the unique case of transition from $y_0$).
\begin{align*}
    A &= [ a_{i,j} ] \quad i,j \in \mathcal{Y}, & \quad & a_{i,j} \coloneqq p(y_t = j \mid y_{t-1} = i), \\
    B  &= [ b_i(k) ] \quad i \in \mathcal{Y}, & \quad  & b_i(k) \coloneqq p(x_t = k \mid y_t = i), \\
    \pi &= [ \pi_i ] \quad i \in \mathcal{Y}, & \quad & \pi_i \coloneqq p(y_1 = i).
\end{align*}
Taken together, these matrices form a complete set of model parameters which we call $\theta = (A,B,\pi)$.
Since the probability distributions we derive from our HMM will be dependent on these parameters we also introduce the notation $\ptheta(\boldX) \coloneqq p(\boldX \mid \theta)$.
By adjusting the components of $\theta$ we are able to tweak the performance of the HMM or even re-purpose it for entirely different tasks.

Having established the HMM, three questions central to enabling its practical application, including for POS tagging, emerge \autocite{rabiner-1989-tutorial}:
\begin{enumerate}
    \item{\textbf{(Computing Marginals)}} Given $\boldX$ and a HMM $\theta = (A,B,\pi)$, how do we efficiently compute $\ptheta(\boldX)$? \label{q:key-q1}
    \item{\textbf{(Sequence Labelling)}} Given $\boldX$ and a HMM $\theta = (A,B,\pi)$, how do we choose a corresponding state sequence $\boldY$ that best explains $\boldX$? \label{q:key-q2}
    \item{\textbf{(Parameter Estimation)}} How might we adjust the model parameters $\theta = (A,B,\pi)$, in order to maximise $\ptheta(\boldX)$? \label{q:key-q3}
\end{enumerate}
These questions respectively motivate the introduction of the forward-backward, Viterbi and Baum-Welch algorithms.

\subsection{The Forward-Backward Algorithm} \label{sec:fb-algo}

The derivation of this algorithm is motivated by the first question, that is, how can we efficiently compute the marginal distribution $\ptheta(\boldX)$? Answering this question will be pivotal in answering the succeeding questions, as we will need to call this calculation as a sub-procedure many times, particularly in parameter estimation.
The marginal distribution can also be of interest itself, for example, a comparison between $\ptheta(\boldX)$ and $p_{\theta'}(\boldX)$ (where $\theta$ and $\theta'$ have been trained separately) could be used to identify the most likely author of a passage $\boldX$.

We first attempt to directly derive $\ptheta(\boldX)$ by noting that it is equivalent to the sum of $\ptheta(\boldX, \boldY)$ over all possible state-sequences. To do so, we require the following two identities that result from the emission independence and Markov assumptions:
\begin{align*}
    \ptheta(\boldX \mid \boldY) &= \prod_{t=1}^T \ptheta(x_t \mid y_t) = \prod_{t=1}^T b_{y_t}(x_t), \\
    \ptheta(\boldY) &= \prod_{t=1}^T \ptheta(y_t) = \pi_{y_1} a_{y_1,y_2} a_{y_2,y_3} \cdots a_{y_{T-2},y_{T-1}} a_{y_{T-1},y_T}.
\end{align*}
Hence, the marginal probability can be expressed
\begin{equation*}
    \ptheta(\boldX) = \sum_{\boldY} \ptheta(\boldX,\boldY) = \sum_{\boldY} \ptheta(\boldX \mid \boldY) \ptheta(\boldY) = \sum_{\boldY} \pi_{y_1} b_{y_1}(x_1) a_{y_1, y_2} b_{y_2}(x_2) \cdots a_{y_{n-1}, y_n} b_{y_n}(x_n).
\end{equation*}
Unfortunately, this expression is intractable, requiring calculations of the order $2T|\mathcal{Y}|^T$, owing to the $2T - 1$ multiplications required for each of the $|\mathcal{Y}|^T$ possible values of $\boldY$.
Such complexity scales immensely quickly with $T$ and $|\mathcal{Y}|$, making it infeasible for practical-use. For example, a model trained on the base tagset of the BNC ($|\mathcal{Y}|=61$) \autocite{bnc-corpus} would require $7.99 \times 10^{19}$ calculations to calculate $\ptheta(\boldX)$ for a sequence of only five words given no prior trimming of the search space of $\boldY$.

In order to calculate in a more efficient way we will define the forward-function, the first of a family of variables which allow for the marginal distribution to be found inductively.
\begin{definition}[Forward-Function] \label{def:forward-func}
    Let $\alpha_t(i)$ denote the joint probability of observing the sequence $\boldX_{1:t} \coloneqq (x_1,x_2,\ldots,x_t)$ $(1 \leq t \leq T)$ and the tag $y_t$ being $i$, given model parameters $\theta$.
    \begin{equation*}
        \alpha_t(i) \coloneqq \ptheta(\boldX_{1:t},y_t=i)
    \end{equation*}
\end{definition}
Having defined $\alpha_t(i)$, we note that $\ptheta(\boldX) = \sum_{i \in \mathcal{Y}} \alpha_T(i)$.
Hence, if we can efficiently compute $\alpha_T(i)$ we will have solved the problem at hand.
Thankfully, the value of any $\alpha$ can be found efficiently by the inductive `forward-algorithm'. The base case $t=1$ is trivial, being the product of the probability of starting at $i$ and the probability of emitting the word $x_1$. Next, note that any $\alpha_t(i)$ can be calculated similarly by taking all states at $t-1$ and multiplying by the transition and emission probabilities that would output $y_{t}=i$ and $x_{t} = x_t$ from each of these, summing them to find the result.
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \alpha_1(i) = \pi_{i} b_i(x_1) \\
        \text{Inductive Step:} & \quad \alpha_{t+1}(j) = \left[ \sum_{i \in \mathcal{Y}} \alpha_{t}(i)a_{i,j} \right] b_j(x_t)
    \end{aligned} \qquad 
        \begin{array}{lr}
            i \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{equation*}
At each step of induction we calculate the value $\alpha_t$ for all $|\mathcal{Y}|$ states, where each is derived from $|\mathcal{Y}|$ previous states.
Carrying out this calculation for each of the $T-1$ steps required to find $\alpha_T(i)$ results an overall complexity of order $T|\mathcal{Y}|^2$.
In real terms, the earlier 5-word example where $|\mathcal{Y}|=61,n=5$ reduces from over $7.99 \times 10^{19}$ calculations to $18605$.

Although the forward-function is sufficient in computing the marginal, we also introduce a mirrored approach: the backward-function.
Whilst trivial at this point, defining both the forward and backward-functions will prove essential in parameter estimation.
\begin{definition}[Backward-Function] \label{def:backward-func}
    Let $\beta_t(i)$ denote the joint probability of observing the sequence $\boldX_{t+1:T}$ $(1 \leq t \leq T)$ and the tag $y_t$ being $j$ given model parameters $\theta$.
    \begin{equation*}
        \beta_t(j) \coloneqq \ptheta(\boldX_{t:T} \mid y_t=k)
    \end{equation*}
\end{definition}
In parallel to the forward-function, $\ptheta(\boldX) = \sum_{j \in \mathcal{Y}} \beta_1(j)$ and the values of $\beta$ can be calculated inductively by the `backward-algorithm', starting at $t=T$ and terminating at $t=1$.
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \beta_T(i) = 1 \\
        \text{Inductive Step:} & \quad \beta_{t}(i) = \sum_{j \in \mathcal{Y}} a_{i,j} b_j(x_{t+1}) \beta_{t+1}(i)
    \end{aligned} \qquad 
        \begin{array}{lr}
            i \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{equation*}
Unsurprisingly, the backward-algorithm makes the same improvements in computational efficiency as the forward-algorithm when computing the marginal distribution $\ptheta(\boldX) = \beta_0(y_0)$.

\subsection{The Viterbi Algorithm}\label{sec:viterbi-algo}

Despite the importance of each of the three key questions for HMMs, the second is perhaps the most clearly pertinent to POS tagging. Indeed, given a HMM pre-trained on words and their parts-of-speech, answering this question will provide us with our desired classifier.
In answering, we must first address how we determine the `best' matched tag-sequence $\boldY$.
We may be tempted, as in the na{\"i}ve Bayes, to assume that the best sequence can be constructed by selecting the most likely tag index by index: $\yhat_t \coloneqq \argmax_{i \in \mathcal{Y}} \ptheta(y_t = i \mid \boldX)$.
The forward and backward functions we have derived make this possible and efficient since together, they account for the entire tag sequence.
The probability we are looking for, that of reaching any one specified tag at index $t$, is equal to the normalised product of the forward and backward functions. It will prove to be a useful value in subsequent calculations and so we denote it as a new function $\gamma_t(i)$.
\begin{equation*}
\gamma_t(i) \coloneqq \ptheta(y_t = i \mid \boldX) = \frac{\alpha_t(i)\beta_t(i)}{\ptheta(\boldX)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j \in \mathcal{Y}}\alpha_t(j) \beta_t(j)}
\end{equation*}
Ultimately, taking such an approach to prediction is misguided: making choices that maximise only the local probability of specific nodes can result in an overall prediction with highly incoherent tags and potentially impossible transitions.
We must instead maximise the holistic probability $\ptheta(\boldY \mid \boldX)$.
As with the marginal distribution, finding this value directly will be far too computationally intensive.

The \textbf{Viterbi algorithm} \autocite{viterbi-1967} is perfectly suited to this task.
The algorithm enables the efficient computation of a most-likely sequence of hidden states given a prior distribution.
In our use case, we wish to compute the most likely `Viterbi path' $\boldY$ given our model parameters $\theta$.
We first define the Viterbi variable $\delta_t(i)$ which returns the maximum probability of any one path length $t$ through the Markov chain, given that we observe $\boldX_{1:t}$ and the path terminates at tag $i$.
\begin{equation*} \label{eq:viterbi-var}
    \delta_t(i) \coloneqq \max_{\boldY_{1:t-1}}\ptheta(\boldY_{1:t-1}, y_t = k, \boldX_{1:t})
\end{equation*}
This value can, again, be found by induction. However, unlike the forward-backward algorithm, the Viterbi algorithm requires that we track the most probable sequence at each step i.e. the largest value of $\delta_t(i)$.
Tracking this variable will allow for the formation of the most likely sequence $\yhat$ by backtracking. For this reason we also define the tracking variable $\phi_t(i)$.
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \begin{array}{lr}
                                        \delta_1(i) = \pi_i b_i(x_1) \\
                                        \phi_1(i) = 0
                                    \end{array} \\
        \text{Inductive Step:} & \quad \begin{array}{lr}
                                            \delta_{t}(j) = \max_i \left[ \delta_{t-1}(i) a_{i,j} \right] b_k(x_t) \\
                                            \phi_t(j) = \argmax_i \left[ \delta_{t-1}(i) a_{i,j} \right]
                                        \end{array}
    \end{aligned} \qquad 
        \begin{array}{lr}
            i,j \in \mathcal{Y} \\
            2 \leq t \leq T
        \end{array}
\end{equation*}
The full algorithm, adapted from \autocite{rabiner-1989-tutorial,eisenstein-nlp-2019}, is given in \cref{alg:Viterbi}.
\begin{algorithm}
\caption{Viterbi Algorithm} \label{alg:Viterbi}
\begin{algorithmic}
\For{$i \in \mathcal{Y}$}
    \State $\delta_1(i) \gets \pi_i b_i(x_1)$
\EndFor
\For{$t \in \{2,\ldots,T\}$}
    \For{$i \in \mathcal{Y}$}
    \State $\delta_t(i) \gets \max_{j \in \mathcal{Y}} \left[ \delta_{t-1}(j) a_{j,i} \right] b_k(x_t)$
    \State $\phi_t(i) \gets \argmax_{j \in \mathcal{Y}} \left[ \delta_{t-1}(j) a_{j,i} \right]$
\EndFor
\EndFor
\State $\yhat_T \gets \argmax_{i \in \mathcal{Y}} \delta_T(i)$
\For{$t \in \{T-1,\ldots,1\}$}
    \State $\yhat_t \gets \phi_{t+1}(\yhat_{t+1})$
\EndFor
\State \Return $\hat{Y}$
\end{algorithmic}
\end{algorithm}
As earlier, applying this dynamic programming results in large gains in cost efficiency. There will be $T \times |\mathcal{Y}|$ Viterbi variables to compute, each of which will require finding a maximum over $|\mathcal{Y}|$ possible previous tags. Overall necessitating $T|\mathcal{Y}|^2)$ calculations to derive all necessary variables and $T$ operations to trace the best matched sequence \autocite{eisenstein-nlp-2019}.

\subsection{The Baum-Welch Algorithm}

Our current answer to the POS tagging problem with HMMs is not quite complete, since we have pre-supposed a fully trained model but have not yet devised methods for estimating the parameters of such a model.
A reasonable starting point is to take the relative frequency counts of transitions and emissions from $\mathcal{D}$ \autocite{eisenstein-nlp-2019}.
Since there are so many possible transitions and emissions it is likely that many will appear very few times, resulting in highly varying, or even zero, probabilities.
As such, if we are to take relative frequency counts as our estimates, it is sensible to counter the resulting variance by introducing smoothing parameters:
\begin{equation} \label{eq:additive-smoothing}
\begin{aligned}
    a_{i,j} &\coloneqq p(y_t = i \mid y_{t-1} = j) \gets \frac{\lambda_A + p(y_t = j,  y_{t-1} = i)}{\lambda_A |\mathcal{Y}| + p(y_{t-1} = k')} \\
    b_i(k) &\coloneqq p(x_t = k \mid y_t = i) \gets \frac{\lambda_B + p(x_t = k ,  y_t = i)}{\lambda_B |\mathcal{Y}| + p(y_t = k)}
\end{aligned} \qquad \lambda_A, \lambda_B \in \mathbb{R}^+
\end{equation}
We use separate hyper-parameters for the transition and emission values as we would expect that the emission probability may need more smoothing due to the word-vocabulary $\mathcal{X}$ being much larger than the set of tags $\mathcal{Y}$ and subsequently more predisposed to over-fitting.
This technique is known as additive (or Laplace) smoothing.

Although this approach has merit, there is no guarantee that it will select the `best' parameters, or even locally optimal ones, instead we will describe an algorithm which is guaranteed to converge: the \textbf{Baum-Welch algorithm} \autocite{baum-1972-alg}.
Baum-Welch is an iterative gradient-descent algorithm and has the desirable property of convergence, although it is not necessarily guaranteed to reach a global minimum.
The Baum-Welch algorithm builds on the forward-backward algorithm, combining it with expectation maximisation to inform the iterative changes made to $\theta$.

Let us first define a final helper variable $\xi_t(i,j)$ which returns the probability of transitioning from tag $i$ at $y_t$ to tag $j$ at $y_{t+1}$ given $\boldX$ and $\theta$.
This probability can be re-written in terms of the pre-defined variables by considering the event it describes in components: firstly reaching tag-state $i$ at $y_t$ given $\boldX_{1:t}$, then transitioning to $j$ from $i$, emitting $x_{t+1}$ and finally reaching tag-state $j$ at $t+1$ given $\boldX_{t+1:T}$. Hence,
\begin{equation*}
\begin{aligned}
    \xi_t(i,j) &\coloneqq \ptheta(y_t = i, y_{t+1} = j \mid \boldX) \\
    &= \frac{\alpha_t(k)a_{i,j}b_j(x_{t+1})\beta_{t+1}(j)}{\ptheta(\boldX)} \\
    &= \frac{\alpha_t(k)a_{i,j}b_j(x_{t+1})\beta_{t+1}(j)}{\sum_{i \in \mathcal{Y}} \sum_{j \in \mathcal{Y}} \alpha_t(i)a_{i,j}b_j(x_{t+1})\beta_{t+1}(j)}
    \end{aligned} \qquad \begin{array}{lr}
            i,j \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{equation*}
We now note that holding $i$ constant and summing $\xi$ over all possible values of $j$ is equivalent to the probability of being in tag-state $i$ at position $t$, which is the same condition that $\gamma$ measures: $\gamma_t(i) = \sum_{j \in \mathcal{Y}} \xi_t(i,j)$.
With $\alpha, \beta, \gamma, \xi$ all defined we can now make the iterative step of the Baum-Welch algorithm by noting that
\begin{itemize}
    \item $\sum_{t=1}^{T-1} \gamma_t(i)$ is the expected number of times that tag $i$ will be visited (as a non-final node) in $\boldY$.
    \item $\sum_{t=1}^{T-1} \xi_t(i, j)$ is the expected number of times that tag $i$ will transition to tag $j$ in $\boldY$.
\end{itemize}
These interpretations give rise to the following iterations:
\begin{align*}
    \pi_k' &= \text{expected frequency of label $i$ at $(t=1)$} = \gamma_1(i), \\
    a_{i,j}' &= \frac{\text{expected frequency of transitions $i \rightarrow j$}}{\text{expected frequency of transitions from $i$}} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)}, \\
    b_i(k)' &= \frac{\text{expected frequency of observing word $k$ at label $i$}}{\text{expected frequency of being at label $i$}} = \frac{\sum_{t=1}^T \gamma_t(i)\ind{x_t = k}}{\sum_{t=1}^{T} \gamma_t(i)}.
\end{align*}
In order to utilise the entire dataset $\mathcal{D}$ we calculate the iterations for all reference sequences and take the average result as our adjusted parameter set $\theta'$.
Since the Baum-Welch algorithm has been proven to be convergent \autocite{baum-1972-alg}, we can re-run the same step again and again until the change in values is as small as a user-decided tolerance.

\subsection{Limitations of HMMs} \label{sec:hmm-limitations}

Although remarkably stronger than the na{\"i}ve Bayes, HMMs of the kind presented here are limited in the information that they can incorporate into their estimations.
This may become an issue as we look to develop our model further by detecting our own useful features, for example suffixes, the word's position in the sequence or even neighbouring words.
Such features would allow our model to make reasonable predictions for words entirely unseen in our data, an impossible task for either the na{\"i}ve Bayes or HMM.

\end{document}