\documentclass[../main.tex]{subfiles}

\begin{document}

The predictive approach we have devised so far is likely to have some success, however it may lose out on accuracy due to the transition assumption.
Particularly as more finely-grained tags are included in the model, significant relationships between the tags are likely to emerge.
For that reason we substitute the transition assumption for the below,

\begin{assumption}[Markov Assumption] \label{ass:markov-assumption}
    The probability of each tag $Y_i$ is dependent (only) on its predecessor $Y_{i-1}$. Thus $p (Y) = \prod_{i=1}^n p (Y_i \mid Y_{i-1})$.
\end{assumption}

Adopted alongside the emission assumption, the Markov assumption specifies a \textbf{hidden Markov model (HMM)}.

\subsection{Overview of HMMs}

HMMs are probabilistic graphs consisting of two parts: a `hidden' (unobserved) Markov chain (which indexes $Y$) and an observed output sequence (which indexes $X$).
Since the model is \textbf{generative} we can understand the graph by the generation story its assumptions give rise to.
%meaning that in modelling the probability distribution they make an assumption of the method in which the underlying data was generated...
%and their relation to the POS tagging problem can be most easily understood by describing the assumptions they make of the way in which the output-sequence $X$ has been generated.
First, the hidden sequence $Y$ is generated by a walk over the Markov chain on $\mathcal{Y}$.
The walk begins at the \textit{start} node $Y_0$ and ends at a pre-determined step $n$.
At each node visited on the walk (excluding \textit{start}), a word is drawn according to a predetermined probability distribution for that specific node and is appended to $X$.
The hypothetical output of such a HMM is illustrated graphically in figure \ref{fig:hmm-diag}.

\subfile{tikz-figures/hmm-diagram}

Within the model there are two key sets of weights to track, transition probabilities (the probability of traversing from one given tag to a specified tag) and emission probabilities (the probability of outputting a specified word given a specific tag).
Note that transition from one node back into itself is permitted as long as an arc was traversed.
These weights can be easily stored in matrices transition $B \in \mathbb{R}^{|\mathcal{Y}| \times |\mathcal{Y}|}$ and emission $A\in \mathbb{R}^{|\mathcal{Y} |\times |\mathcal{W}|}$:
\begin{gather*} \label{eqn:weight-matrix}
    T  = \left[ p(Y_t = k \mid Y_{t-1} = k') \right]_{k',k}, \quad T_i(j) \\
    E = [p(X_t = w \mid Y_t = k)]_{k,w}, \quad E_i(j) \\
    \pi = [p(Y_1 = k)]_{k}
\end{gather*}

%For notational convenience, a function $E_i(w)$ is defined such that $E_i(w) = E_{i, j}$ where $w = \mathcal{W}_j$.
The vector $\pi$ has also been defined and = describes the probability that the sequence $Y$ starts with any given tag. This could be included within $E$ using the \textit{start} node, however I have avoided doing so as the \textit{start} node requires unique consideration such as being able to be visited once only.
All together these form the model parameters $\theta = (T,E,\pi)$ which we will later tweak in order to optimise our prediction ability for the given task.

HMMs are relevant to other tasks, such as speech recognition and RNA sequencing. By adjusting the choices of $\theta$, $\mathcal{Y}$ and the vocabulary that $w$ draws from, models can be created for any of these applications.

\subsubsection{Key Questions for HMMs}

Having established the structure, there are three questions central to enabling the practical usage, including POS tagging, of HMMs \autocite{rabiner-1989-tutorial}:
\begin{enumerate}
    \item Given the observed sequence $X = X_1 X_2 \cdots X_n$ and a hidden Markov model $\theta = (T, E, \pi)$, how do we efficiently compute $p(X \mid \theta)$ \label{q:key-q1}
    \item Given the observed sequence $X = X_1 X_2 \cdots X_n$ and a hidden Markov model $\theta = (T, E, \pi)$, how do we choose a corresponding state sequence $Y = Y_1 Y_2 \cdots Y_n$ that best explains the observations? \label{q:key-q2}
    \item How might we adjust the model parameters $\theta = (T, E, \pi)$, in order to maximise $p(X \mid \theta)$? \label{q:key-q3}
\end{enumerate}
These questions will motivate the introduction of the forward-backward, Viterbi and Baum-Welch algorithms. Each algorithm is essential for the derivation and usage of a hidden Markov model for POS tagging.

\subsection{Forward-Backward Algorithm}

The derivation of this algorithm is motivated by question \ref{q:key-q1} and will also be pivotal in answering the succeeding questions.
In addition, the forward-backward algorithm can be adapted to offer some unique practical applications of its own such as identifying the most likely author of a given text.

We first look to directly derive $p(X \mid \theta)$ by imagining all possible state-sequences $Y$, then finding all joint probabilities $p(X, Y \mid \theta)$ and finally summing these values back over all $Y$. In order to do so we require the following two identities that result from the model assumptions:
\begin{alignat*}{2}
    p(X \mid Y, \theta) &= \prod_{i=1}^n p(X_i \mid Y_i, \theta) = \prod_{i=1}^n E_{Y_i,X_i} \quad & \text{\small by assumption \ref{ass:emission}} \\
    p(Y \mid \theta) &= \pi_{Y_1} T_{Y_1,Y_2} \cdots T_{Y_{n-1},Y_n} \quad & \text{\small by assumption \ref{ass:markov-assumption}}
\end{alignat*}
These identities can then be applied as follows,
\begin{align*}
    p(X \mid \theta) &= \sum_{\forall \: Y} p(X,Y \mid \theta) = \sum_{\forall \: Y} p(X \mid Y, \theta) p(Y \mid \theta) \\
    &= \sum_{\forall \: Y} \pi_{Y_1} E_{Y_1, X_1} T_{Y_1, Y_2} E_{Y_2, X_2} \cdots T_{Y_{n-1}, Y_n} E_{Y_n, X_n} \label{eq:intractable-forward}
\end{align*}

The calculation given by equation \ref{eq:intractable-forward} is intractable and has an overall complexity of order $O(2n|\mathcal{Y}|^n)$, owing to the $2n - 1$ multiplications for each of the $|\mathcal{Y}|^n$ possible values of $Y$.
Such complexity scales immensely quickly making it infeasible for practical-use. For example, a model trained on the base tagset of the BNC ($|\mathcal{Y}|=61$) \autocite{bnc-corpus} would require $7.99 \times 10^{19}$ calculations to calculate $p(X \mid \theta)$ for a sequence of only five words.

In order to calculate in a more efficient way we take an inductive approach: the forward-backward algorithm.
Firstly, we define a helper function that will simplify subsequent calculations.

\begin{definition}[Forward-Function] \label{def:forward-func}
    Let $\alpha_t(k)$ denote the joint probability of observing the sequence $X_1 X_2 \cdots X_t \, (1 \leq t \leq n$) and the tag $Y_t$ being $k$, given model parameters $\theta$.
    \begin{equation*}
        \alpha_t(k) \coloneqq p (X_1,X_2,\ldots,X_t,Y_t=k \mid \theta)
    \end{equation*}
\end{definition}

Having defined $\alpha_t(k)$, we note that $p (X \mid \theta) = \sum_{k \in \mathcal{Y}} \alpha_n(k)$.
The value of $\alpha_t(k)$ can be found by the `forward-algorithm' which applies induction, starting at $t=1$ and repeatedly applying the inductive step (\ref{eq:forward-inductive}) until the desired value of $t$ is reached.

\begin{gather*}
    \text{Base Case:} \quad \alpha_1(k) = \pi_{k} E_{k,X_1} \qquad k \in \mathcal{Y} \\
    \text{Inductive Step:} \quad \alpha_{t+1}(k) = \left[ \sum_{k' \in \mathcal{Y}} \alpha_{t}(k')T_{X_t,X_{t+1}} \right] E_{k,X_{t+1}} \qquad
        \begin{array}{lr}
            1 \leq t \leq n-1\\
            k, k' \in \mathcal{Y}
        \end{array} \label{eq:forward-inductive}
\end{gather*}

At each step of induction we calculate the value $\alpha_t$ for all $|\mathcal{Y}|$ states, where each is derived from $|\mathcal{Y}|$ previous states.
Carrying out this calculation for each of the $n-1$ steps results in overall complexity of order $O(n|\mathcal{Y}|^2)$.
In real terms, the earlier example with parameters $|\mathcal{Y}|=61,n=5$ goes from over $7.99 \times 10^{19}$ calculations to $18605$. Most real-world examples will have much higher $n$ values and so savings in efficiency that are orders of magintude beyond even this.

The backward-function can be viewed as a mirrored approach to this problem, although seemingly trivial at this point it has important application in the solution to question 3.

\begin{definition}[Backward-Function] \label{def:backward-func}
    Let $\beta_t$ denote the joint probability of observing the sequence $X_t X_{t+1} \cdots X_n \, (1 \leq t \leq n)$ and the tag $Y_t$ being $k$ given model parameters $\theta$.
    \begin{equation*}
        \beta_t(k) \coloneqq p (X_{t+1},X_{t+2},\ldots,X_n \mid Y_t=k, \theta)
    \end{equation*}
\end{definition}

Similarly to the forward-function, note that $p (X \mid \theta) = \sum_{k \in \mathcal{Y}} \beta_1(k)$. Which can be calculated by the `backward-algorithm', starting at $t=n$ and terminating at $t=1$.
\begin{gather*}
    \text{Base Case:} \quad \beta_n(k) = 1 \qquad k \in \mathcal{Y} \\
    \text{Inductive Step:} \quad \beta_{t}(k) = \sum_{k' \in \mathcal{Y}} T_{Y_t,k} E_{k,X_{t+1}} \beta_{t+1}(k') \qquad
        \begin{array}{lr}
            1 \leq t \leq n-1\\
            k,k' \in \mathcal{Y}
        \end{array}
\end{gather*}
Unsurprisingly, the backward-function makes the same improvements in computational efficiency as the forward-function.

\subsection{Viterbi Algorithm}

Question \ref{q:key-q2} is directly equivalent to solving the central POS tagging problem itself (with a given $\theta$).
To answer it we must first address how to measure what is the `best' matched tag-sequence $Y$ for word-sequence $X$ and model $\theta$.
We may be tempted, much like in section \ref{sec:basic-model} to select the tag that is most likely at each index in $Y$.
In fact, the forward and backward functions we have already derived make this much simpler.
%If we define this probability using a third helper function $\gamma_t(i) = p (Y_t = \mathcal{Y}_i \mid X, \theta)$.
Between the forward and backward function the entire tag sequence is accounted for.
Hence, the probability of reaching any one specified tag at point $t$ in the sequence is equal to the forward function multiplied by the backward function and normalised over all tags. We denote this value $\gamma_t(k)$ for later use.
\begin{equation*}
\gamma_t(k) \coloneqq p (Y_t = k \mid X, \theta) = \frac{\alpha_t(k)\beta_t(k)}{p(X \mid \theta)} = \frac{\alpha_t(k)\beta_t(k)}{\sum_{k' \in \mathcal{Y}}\alpha_t(k') \beta_t(k')}
\end{equation*}
However, as earlier, taking such an approach to predict $Y$ is misguided.
By considering only the probability of individual nodes the \textbf{overall} probability of the generated sequence can suffer, in the extreme resulting in paths that are impossible due to transition probabilities of zero.
Instead we wish to maximise the probability of the entire sequence and utilise the HMM's sequential structure more thoroughly.
Much like earlier calculations, finding the set of all probabilities directly is far too computationally intensive and we will be required to devise a dynamic programming approach.

The \textbf{Viterbi algorithm} \autocite{viterbi-1967} is perfectly suited for this task, being able to obtain an estimate for $Y$ that maximises the conditional probability given $X$.
In addition, the Viterbi algorithm is easily adapted for the more advanced models to be discussed later.

First we define the \textbf{Viterbi variable} $v_t(k)$ which returns the maximum probability of any one path length $t$ through the Markov chain, given that we observe $X_1$ to $X_t$ and the path terminates at tag $k$.
\begin{equation*}\label{eq:viterbi-var}
    v_t(i) \coloneqq \max_{Y_1, \ldots, Y_{t-1}} p(Y_1, \ldots, Y_{t-1}, Y_t = k, X_1, \ldots, X_t \mid \theta)
\end{equation*}

This value can, again, be found by induction.
\begin{gather*}
    \text{Base Case:} \quad v_1(k) = \pi_k E_{k, X_1} \qquad k \in \mathcal{Y} \\
    \text{Inductive Step:} \quad v_{t}(k) = max_{k'} \left[ v_{t-1}(k') T_{k',k}\right] \cdot E_{k,X_t} \qquad
        \begin{array}{lr}
            2 \leq t \leq n\\
            k \in \mathcal{Y}
        \end{array}
\end{gather*}
The difference in application of the Viterbi variable as opposed to $\alpha$ and $\beta$ is that we must keep track of the sequence that is currently providing the best match.
By tracking the tag that maximises $v_t(k)$ at each step we are able to traverse the path backwards and find the optimal tag-sequence.

We will define the tracking variable $b_t(k)$ with base case $b_1(k)=0$ and planning to update it at each inductive step in the same manner as the Viterbi variable but taking the $\argmax$ as opposed to $\max$.

The full algorithm is described below as given in \autocite{eisenstein-nlp-2019}:
\begin{algorithm}
\caption{Viterbi Algorithm} \label{alg:Viterbi}
\begin{algorithmic}
\For{$k \in \mathcal{Y}$}
    \State $v_1(k) \gets \pi_k E_{k, X_1}$
\EndFor
\For{$t \in \{2,\ldots,n\}$}
    \For{$k \in \mathcal{Y}$}
    \State $v_t(k) \gets \max_{k' \in \mathcal{Y}} \left[ v_{t-1}(k') T_{k',k} \right] \cdot E_{k,X_t}$
    \State $b_t(i) \gets \argmax_{k' \in \mathcal{Y}} \left[ v_{t-1}(k') T_{k',k} \right] \cdot E_{k,X_t}$
\EndFor
\EndFor
\State $\hat{Y}_n \gets \argmax_{k \in \mathcal{Y}} v_n(k)$
\For{$t \in \{n-1,\ldots,1\}$}
    \State $\hat{Y}_t \gets b_m(\hat{Y}_{t+1})$
\EndFor
\State \Return $\hat{Y}$
\end{algorithmic}
\end{algorithm}

%The Viterbi algorithm applies the principle of dynamic programming to prune the traversal tree of the HMM as we progress, in this way it removes the need for many recalculations and is a vast improvement in cost complexity compared to the brute force approach.
%The Viterbi Algorithm was developed outside of the context of natural language understanding but is of great importance in sequential labelling.
%The algorithm is a form of dynamic programming and vastly improves efficiency of classification by reducing the need for repeat calculations and terminating fruitless traversal paths early.
%When reaching a node via two paths, discontinue the path that reached with a lower probability, from here on out probability will continue to always be lower!

%TODO WORK IN NATURALLY
%Such models were initially devised by %\autocite{jelinek-1975-hmm}.

%The Markov chain itself is a directed and weighted graph, where the weight of the arc from node $A$ to node $B$ is given by the \textit{transition probability}, defined as $p (C_k=B \mid C_{k-1}=A)$. In this context the nodes of the chain represent every word $w$ within our lexicon, without repetition.
%By walking the Markov chain, at each node selecting the next node by the probability given according to it;s weight we can generate a sequence of syntactic categories $\mathscr{C}$. This illustrates the Markov assumption that the future of the chain is dependent only on the current node and none of its predecessors.
%Finally, each node in the chain is linked to a corresponding observed state, once more by a predefined probability, now known as the \textit{output probability}.
%Utilising Markov models allows us to consider the preceding word as a parameter in our estimate of category.
%Each node we reach within a Markov model is associated with every other node by a weighted arc with the weight representing the probability that we traverse to that node next.
%We can generate sequences from the model by traversing this network of nodes.
%The concept of a hidden Markov model is especially apt in our problem context, in this extension of the Markov chain we observe only the state of the node whilst the  underlying value that this state is probabilistically dependent on is hidden to us. Here we can observe that $w$ is our observed state, and $c$ the hidden state we wish to solve for.
%In doing so we take bigrams of the text, a concept which is extendable to the idea of $N$-grams, which appear frequently in Natural Language Processing literature and techniques.

As earlier, applying this dynamic programming results in large gains in cost efficiency. There will be $n \times |\mathcal{Y}|$ Viterbi variables to compute, each of which will require finding a maximum over $|\mathcal{Y}|$ possible previous tags. Thus, requiring $O(n|\mathcal{Y}|^2)$ calculations to derive all necessary variables and $O(n)$ operations to trace the best matched sequence $Y$ \autocite{eisenstein-nlp-2019}.
%Trellis diag?

\subsection{The Baum-Welch Algorithm}

Our conclusion to the POS tagging problem is not quite complete, since the Viterbi algorithm requires a trained model with parameters $\theta$ to work but we are yet to discuss how to obtain reasonable values for these  parameters.
This is the objective of question \ref{q:key-q3}.

For this we can apply the same principles as for the first model, taking relative frequency counts of transitions and emissions from a tagged corpus.
As with the $n$-gram model, this is likely to result in highly varying probabilities as well as many unreasonable zero probabilities.
We can take the same approach to counter this variance as we did then, introducing smoothing parameters $\theta_T$ and $\theta_E$.
\begin{gather*}
    p (Y_i = k \mid Y_{i-1} = k') \gets \frac{\theta + \countfunc (Y_i = k ,  Y_{i-1} = k')}{\theta_T |\mathcal{Y}| + \countfunc (Y_{i-1} = k')} \\
    p (X_i = w \mid Y_i = k) \gets \frac{\theta_E + \countfunc (X_i = w ,  Y_i = k)}{\theta_E |\mathcal{Y}| + \countfunc (Y_i = k)}
\end{gather*}

It is sensible to use separate hyper-parameters for each as we would expect that the emission probability may need more smoothing due to the word-vocabulary being much larger than $\mathcal{Y}$ and subsequently at higher risk of being overfit.

Although this approach has merit, a more sophisticated method the Baum-Welch algorithm \autocite{baum-1972-alg} is recommended for use in robust POS HMMs \autocite{kupiec-1992-hmm}.
The Baum-Welch algorithm, like the Viterbi, can also be adapted for training other model types.
It is an iterative gradient-descent algorithm and has the desirable property of convergence, although it is not guaranteed to reach a global minimum.

The Baum-Welch algorithm builds on the forward-backward algorithm, combining it with expectation maximisation to provide a most likely $Y$ given $X$.
In this context, $Y$ will not be denoting a single, short sequence but instead an entire corpus to be used for model training. The results of the Baum-Welch algorithm can be derived for many sequences in this corpus and then averaged at each step of iteration.

Let us first define one last helper function $\delta_t(k, k')$ which returns the probability of transitioning from tag $k$ to tag $k'$ given $X$ and $\theta$. The actual value is now easily derived using the arsenal of other functions we have defined. Consider this as the probability of reaching state $k$ at $t$ given observations $X_1$ to $X_t$, then the probability of transitioning to $k'$ and emitting $X_{t+1}$ and finally the probability of reaching state $k'$ at $t+1$ given $X_{t+1}$ to $X_n$:
\begin{align*}
    \delta_t(k, k') &\coloneqq p(Y_t = k, Y_{t+1} = k' \mid X, \theta) \qquad k,k' \in \mathcal{Y} \\
    &= \frac{\alpha_t(k)T_{k,k'}E_{k',X_{t+1}}\beta_{t+1}(k')}{p(X \mid \theta)} \\
    &= \frac{\alpha_t(k)T_{k,k'}E_{k',X_{t+1}}\beta_{t+1}(k')}{\sum_{k \in \mathcal{Y}} \sum_{k' \in \mathcal{Y}} \alpha_t(k)T_{k,k'}E_{k',X_{t+1}}\beta_{t+1}(k')}
\end{align*}
Noting that holding the $k$ argument of $\delta$ constant and summing $k'$ over all possible values is equivalent to simply the probability of being in tag-state $k$ at position $t$ (or vice versa with $t+1$):
\begin{equation*}
    \gamma_t(k) = \sum_{k' \in \mathcal{Y}} \delta_t(k,k')
\end{equation*}

With all functions $\alpha, \beta, \gamma, \delta$ now defined we are able to make estimates of how the parameters should be changed at each step of the Baum-Welch algorithm.
Notice that:
\begin{itemize}
    \item $\sum_{t=1}^{n-1} \gamma_t(k)$ is the expected number of times that tag $k$ will be visited (as a non-final node) in $Y$.
    \item $\sum_{t=1}^{n-1} \delta_t(k, k')$ is the expected number of times that tag $k$ will transition to tag $k'$ in $Y$.
\end{itemize}
Thanks to these interpretations it becomes much easier to see how the values of $T$, $E$ and $\pi$ should be adjusted.
\begin{gather*}
    \pi_k ' = \gamma_1(k) \\
    T_{k,k'}' = \frac{\text{expected \# transitions $k \rightarrow k'$}}{\text{expected \# transitions from $k$}} = \frac{\sum_{t=1}^{n-1} \delta_t(k, k')}{\sum_{t=1}^{n-1} \gamma_t(k)} \\
    E_{k,w}' = \frac{\text{expected \# times tag = $k$ and observing $w$}}{\text{expected \# times tag = $k$}} = \frac{\sum_{t \in T_w} \gamma_t(k)}{\sum_{t=1}^{n} \gamma_t(k)}
\end{gather*}
Where $T_w = \{t \in \{1,\ldots,n\} \mid X_t = w\}$.

Since the Baum-Welch algorithm is iterative and convergent the same algorithm can be run again using the new $\theta' = (T', E', \pi')$ and then again, repeatedly until small to no change in values is made.

\subsection{Limitations}

Although much stronger than the first model and likely to have a high accuracy rate HMMs are limited in the information that can be incorporated into the predictions they make.

This may arise as an issue when aiming to incorporate information beyond what is directly specified in our corpus, such as detecting our own features (e.g. suffixes) which can allow our model to make reasonable predictions for words not previously seen and is especially helpful for finely-grained tags such as. distinctions between singular and plural.
As mentioned earlier the Viterbi algorithm is able to incorporate such additional feature information, even as we move beyond the scope of a hidden Markov model.

We cannot simply incorporate features such as suffixes as another output layer as they clearly go against the independence of observations assumption ($p(w = \textit{`satisfy'}$ and $p(\text{suffix} = \textit{`-ify'})$ are clearly dependent variables).

\end{document}