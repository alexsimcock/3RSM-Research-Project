\documentclass[../main.tex]{subfiles}

\begin{document}

As we look to build upon the na{\"i}ve Bayes approach we might first consider reworking the restrictive transition assumption.
Intuitively, an adjective or noun tag should be more likely to succeed the determiner `the' than a verb.
We would expect that as more finely-grained tags are included in the model, many such significant relationships between the tags are likely to emerge.
For this reason we substitute the transition assumption for a Markov assumption.

\begin{assumption}[Markov Assumption] \label{ass:markov-assumption}
    The probability of each tag $Y_t$ is dependent (only) on its predecessor $Y_{t-1}$. Thus $p(Y) = \prod_{t=1}^T p(Y_t \mid Y_{t-1})$. Under this assumption $Y$ is considered a stochastic process that exhibits the Markov property.
\end{assumption}

We also introduce a `start' node $Y_0$, which will allow the calculations defined by the Markov assumption to extend to $Y_1$.
Adopted alongside the emission assumption, the Markov assumption specifies a \textbf{hidden Markov model (HMM)}.
The HMM can be thought of as a sequential adaptation of the na{\"i}ve Bayes model where not only the observation $X_t$, but also the succeeding tag $Y_{t+1}$, is probabilistically determined by the tag $Y_t$.
The connection is particularly apparent when viewing the models graphically, as in \cref{fig:nb-and-hmm-diag}.

\subfile{tikz-figures/nb-hmm-diagrams}

The node sets of such graphs is the union of the label set $\mathcal{Y}$ and all the words that could appear in $X$.
The directed edges indicate either transition from one tag to another, or the emission of an observed word from a tag and are weighted by the probability of emitting/transitioning to the destination node given the current one.
Therefore, we can understand the sub-graph which contains only the label nodes as a Markov chain on $\mathcal{Y}$ and $Y$ as being generated by a walk over this chain that starts at $Y_0$ and ends after $T$ steps.

%At each node visited on the walk (excluding \textit{start}), a word is drawn according to a predetermined probability distribution for that specific node and is appended to $X$.
%As the graphical model makes clear, our model can be summarised by two key sets of weights, the transition probabilities (the probability of traversing from one given tag to a specified tag) and emission probabilities (the probability of outputting a specified word given a specific tag).
As the graphical representation elucidates, the HMM can be summarised entirely by its transition and emission probabilities, which we store in the matrices $A$, $B$ and $\pi$ (where the vector $\pi$ is the special case of transition from the $Y_0$).
\begin{align*}
    A &= [ a_{i,j} ] \quad i,j \in \mathcal{Y}, & \quad & a_{i,j} \coloneqq p(Y_t = j \mid Y_{t-1} = i), \\
    B  &= [ b_i(k) ] \quad i \in \mathcal{Y}, & \quad  & b_i(k) \coloneqq p(X_t = k \mid Y_t = i), \\
    \pi &= [ \pi_i ] \quad i \in \mathcal{Y}, & \quad & \pi_i \coloneqq p(Y_1 = i).
\end{align*}
Taken together, these matrices form the complete set of model parameters which we call $\theta = (A,B,\pi)$.
By adjusting $\theta$ we are able to tweak the performance of the HMM or even develop HMMs for entirely different tasks, such as speech recognition or gene prediction.
At this point, we also introduce the notation $\ptheta(X) \coloneqq p(X \mid \theta)$ to reflect that the probability estimates we find from our HMM will be conditional on the chosen parameters.

Having established the HMM, three questions central to enabling its practical application, including for POS tagging, emerge \autocite{rabiner-1989-tutorial}:
\begin{enumerate}
    \item{\textbf{(Computing Marginals)}} Given $X$ and a HMM $\theta = (A,B,\pi)$, how do we efficiently compute $\ptheta(X)$? \label{q:key-q1}
    \item{\textbf{(Sequence Labelling)}} Given $X$ and a HMM $\theta = (A,B,\pi)$, how do we choose a corresponding state sequence $Y$ that best explains $X$? \label{q:key-q2}
    \item{\textbf{(Parameter Estimation)}} How might we adjust the model parameters $\theta = (A,B,\pi)$, in order to maximise $\ptheta(X)$? \label{q:key-q3}
\end{enumerate}
These questions respectively motivate the introduction of the forward-backward, Viterbi and Baum-Welch algorithms.

\subsection{Forward-Backward Algorithm}

The derivation of this algorithm is motivated by the first question, that is, how can we efficiently compute the marginal distribution $\ptheta(X)$? Answering this question will be pivotal in answering the succeeding questions, as we will need to call this calculation as a sub-procedure many times.
The marginal distribution can also be of interest itself, for example, a comparison between $\ptheta(X)$ and $p_{\theta'}(X)$ (where $\theta$ and $\theta'$ were trained separately) could be used to identify the most likely author of a passage $X$.

We first attempt to directly derive $\ptheta(X)$ by noting that it is equivalent to the sum of $\ptheta(X, Y)$ over all possible state-sequences. In order to do so we require the following two identities that result from the assumption of emission independence and Markov assumption:
\begin{equation*}
    \ptheta(X \mid Y) = \prod_{t=1}^T \ptheta(X_t \mid Y_t) = \prod_{t=1}^T b_{Y_t}(X_t), \qquad
    \ptheta(Y) = \pi_{Y_1} a_{Y_1,Y_2} a_{Y_2,Y_3} \cdots a_{Y_{T-2},Y_{T-1}} a_{Y_{T-1},Y_T}
\end{equation*}
Which we apply to find the marginal probability:
\begin{equation*}
    \ptheta(X) = \sum_{\forall \: Y} \ptheta(X,Y) = \sum_{\forall \: Y} \ptheta(X \mid Y) \ptheta(Y) = \sum_{\forall \: Y} \pi_{Y_1} b_{Y_1}(X_1) a_{Y_1, Y_2} b_{Y_2}(X_2) \cdots a_{Y_{n-1}, Y_n} b_{Y_n}(X_n)
\end{equation*}
Unfortunately, this calculation is intractable and has complexity of order $O(2n|\mathcal{Y}|^n)$, owing to the $2T - 1$ multiplications required for each of the $|\mathcal{Y}|^T$ possible values of $Y$.
Such complexity scales immensely quickly with $T$ and $|\mathcal{Y}|$, making it infeasible for practical-use. For example, a model trained on the base tagset of the BNC ($|\mathcal{Y}|=61$) \autocite{bnc-corpus} would require $7.99 \times 10^{19}$ calculations to calculate $\ptheta(X)$ for a sequence of only five words given no prior trimming of the search space of $Y$.

In order to calculate in a more efficient way we will define the forward-function, the first of a family of helper functions which will simplify calculations by enabling us to apply induction.
\begin{definition}[Forward-Function] \label{def:forward-func}
    Let $\alpha_t(i)$ denote the joint probability of observing the sequence $X_1, X_2, \ldots, X_t \, (1 \leq t \leq T$) and the tag $Y_t$ being $i$, given model parameters $\theta$.
    \begin{equation*}
        \alpha_t(i) \coloneqq \ptheta(X_1,X_2,\ldots,X_t,Y_t=i)
    \end{equation*}
\end{definition}
Having defined $\alpha_t(i)$, we note that $\ptheta(X) = \sum_{i \in \mathcal{Y}} \alpha_T(i)$.
Hence, if we can efficiently compute $\alpha_T(i)$ we will have solved the problem at hand.
Thankfully, the value of any $\alpha_t(i)$ can be found efficiently by the inductive `forward-algorithm', which calculates $\alpha_{t+1}(j) = \ptheta(X_1,\ldots,X_{t+1},Y_t=j)$ by summing the probabilities of transitioning from $i$ to $j$ and then emitting $X_{t}$ across all possible tags $i$.
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \alpha_1(i) = \pi_{i} b_i(X_1) \\
        \text{Inductive Step:} & \quad \alpha_{t+1}(j) = \left[ \sum_{i \in \mathcal{Y}} \alpha_{t}(i)a_{i,j} \right] b_j(X_t)
    \end{aligned} \qquad 
        \begin{array}{lr}
            i \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{equation*}
At each step of induction we calculate the value $\alpha_t$ for all $|\mathcal{Y}|$ states, where each is derived from $|\mathcal{Y}|$ previous states.
Carrying out this calculation for each of the $T-1$ steps required to find $\alpha_T(i)$ results in overall complexity of order $O(n|\mathcal{Y}|^2)$.
In real terms, the earlier 5-word example where $|\mathcal{Y}|=61,n=5$ reduces from over $7.99 \times 10^{19}$ calculations to $18605$. Most real-world examples will have much higher $T$ values and so will encounter savings in efficiency orders of magnitude even greater than this.

Although the forward-function is sufficient in solving our problem, we also introduce the backward-function which acts as a mirrored approach.
Whilst trivial at this point, defining both the forward and backward-algorithms will prove essential in parameter estimation.
\begin{definition}[Backward-Function] \label{def:backward-func}
    Let $\beta_t(i)$ denote the joint probability of observing the sequence $X_t,X_{t+1},\ldots,X_T \, (1 \leq t \leq T)$ and the tag $Y_t$ being $j$ given model parameters $\theta$.
    \begin{equation*}
        \beta_t(j) \coloneqq \ptheta(X_{t+1},X_{t+2},\ldots,X_T \mid Y_t=k)
    \end{equation*}
\end{definition}
In parallel to the forward-function, $\ptheta(X) = \sum_{j \in \mathcal{Y}} \beta_1(j)$ which can be calculated by the `backward-algorithm', starting at $t=T$ and terminating at $t=1$.
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \beta_T(i) = 1 \\
        \text{Inductive Step:} & \quad \beta_{t}(i) = \sum_{j \in \mathcal{Y}} a_{i,j} b_j(X_{t+1}) \beta_{t+1}(i)
    \end{aligned} \qquad 
        \begin{array}{lr}
            i \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{equation*}
Unsurprisingly, the backward-function makes the same improvements in computational efficiency as the forward-function.

\subsection{Viterbi Algorithm}

Despite the importance of each of the three key questions for HMMs, the second is perhaps the most clearly pertinent to POS tagging. Indeed, given a HMM pre-trained on words and their parts-of-speech, answering this question will provide us with our desired classifier.
In answering, we must first address how we determine the `best' matched tag-sequence $Y$.
We may be tempted, as in the na{\"i}ve Bayes, to assume that the best sequence can be constructed by selecting the most likely tag index by index: $\yhat_t \coloneqq \argmax_{i \in \mathcal{Y}} \ptheta(Y_t = i \mid X)$.
The forward and backward functions we have derived make this possible and efficient since together, they account for the entire tag sequence.
In fact, the probability of reaching any one specified tag at index $t$ is equal to the normalised product of the forward and backward functions, it will prove to be useful in subsequent calculations and so we denote it as a new function $\gamma_t(i)$.
\begin{equation*}
\gamma_t(i) \coloneqq \ptheta(Y_t = i \mid X) = \frac{\alpha_t(i)\beta_t(i)}{\ptheta(X)} = \frac{\alpha_t(i)\beta_t(i)}{\sum_{j \in \mathcal{Y}}\alpha_t(j) \beta_t(j)}
\end{equation*}
However, taking such an approach to prediction is misguided: making choices that maximise only the local probability of specific nodes can result in an overall prediction with highly incoherent tags and potentially impossible transitions.
We must instead maximise the holistic probability $\ptheta(Y \mid X)$, however like earlier calculations, finding this value directly will be far too computationally intensive.

The \textbf{Viterbi algorithm} \autocite{viterbi-1967} is perfectly suited to this task. Whilst the algorithm was originally proposed for use in error-checking codes, its application to HMMs may now be the pre-eminent example.
We first define the \textbf{Viterbi variable} $\delta_t(i)$ which returns the maximum probability of any one path length $t$ through the Markov chain, given that we observe $X_1$ to $X_t$ and the path terminates at tag $i$.
\begin{equation*} \label{eq:viterbi-var}
    \delta_t(i) \coloneqq \max_{Y_1, \ldots, Y_{t-1}}\ptheta(Y_1, \ldots, Y_{t-1}, Y_t = k, X_1, \ldots, X_t)
\end{equation*}
This value can, again, be found by induction. However, unlike $\alpha$ and $\beta$ we will be required to track the most probable sequence at each step (the largest $\delta_t(i)$) as the Viterbi algorithm forms the path by backtracking. For this reason we also define the tracking variable $\phi_t(i)$.
\begin{equation*}
    \begin{aligned}
        \text{Base Case:} & \quad \begin{array}{lr}
                                        v_1(i) = \pi_i b_i(X_1) \\
                                        \phi_1(i) = 0
                                    \end{array} \\
        \text{Inductive Step:} & \quad \begin{array}{lr}
                                            v_{t}(j) = \max_i \left[ v_{t-1}(i) a_{i,j} \right] b_k(X_t) \\
                                            \phi_t(j) = \argmax_i \left[ \delta_{t-1}(i) a_{i,j} \right]
                                        \end{array}
    \end{aligned} \qquad 
        \begin{array}{lr}
            i \in \mathcal{Y} \\
            2 \leq t \leq T
        \end{array}
\end{equation*}
The full algorithm, adapted from \autocite{eisenstein-nlp-2019, rabiner-1989-tutorial}, is given:
\begin{algorithm}
\caption{Viterbi Algorithm} \label{alg:Viterbi}
\begin{algorithmic}
\For{$i \in \mathcal{Y}$}
    \State $\delta_1(i) \gets \pi_i b_i(X_1)$
\EndFor
\For{$t \in \{2,\ldots,T\}$}
    \For{$i \in \mathcal{Y}$}
    \State $\delta_t(i) \gets \max_{j \in \mathcal{Y}} \left[ v_{t-1}(j) a_{j,i} \right] b_k(X_t)$
    \State $\phi_t(i) \gets \argmax_{j \in \mathcal{Y}} \left[ v_{t-1}(j) a_{j,i} \right]$
\EndFor
\EndFor
\State $\hat{Y}_T \gets \argmax_{i \in \mathcal{Y}} v_T(i)$
\For{$t \in \{T-1,\ldots,1\}$}
    \State $\hat{Y}_t \gets \phi_{t+1}(\hat{Y}_{t+1})$
\EndFor
\State \Return $\hat{Y}$
\end{algorithmic}
\end{algorithm}
As earlier, applying this dynamic programming results in large gains in cost efficiency. There will be $n \times |\mathcal{Y}|$ Viterbi variables to compute, each of which will require finding a maximum over $|\mathcal{Y}|$ possible previous tags. Thus, requiring $O(n|\mathcal{Y}|^2)$ calculations to derive all necessary variables and $O(n)$ operations to trace the best matched sequence $Y$.

\subsection{The Baum-Welch Algorithm}

Our conclusion to answering the POS tagging problem with HMMs is not quite complete, since the Viterbi algorithm requires a trained model with parameters $\theta$ to work but we are yet to discuss how to estimate reasonable values for these parameters.

A reasonable starting point will take the relative frequency counts of transitions and emissions from a tagged dataset as its parameters.
This is likely to result in highly varying probabilities as well as many unreasonable zero probabilities and so it is sensible to counter this variance by introducing smoothing parameters for each.
\begin{gather*}
    a_{i,j} \coloneqq p(Y_t = i \mid Y_{t-1} = j) \gets \frac{\lambda_A + \countfunc (Y_t = j,  Y_{t-1} = i)}{\lambda_A |\mathcal{Y}| + \countfunc (Y_{t-1} = k')} \\
    b_i(k) \coloneqq p(X_t = k \mid Y_t = i) \gets \frac{\lambda_B + \countfunc (X_t = k ,  Y_t = i)}{\lambda_B |\mathcal{Y}| + \countfunc (Y_t = k)}
\end{gather*}
It is sensible to use separate hyper-parameters as we would expect that the emission probability may need more smoothing due to the word-vocabulary being much larger than $\mathcal{Y}$ and subsequently at higher risk of being overfit.

Although this approach has merit, a more sophisticated method known as the Baum-Welch algorithm \autocite{baum-1972-alg} is recommended for use in robust HMMs \autocite{kupiec-1992-hmm}.
It is an iterative gradient-descent algorithm and has the desirable property of convergence, although it is not guaranteed to reach a global minimum.
The Baum-Welch algorithm builds on the forward-backward algorithm, combining it with expectation maximisation to provide a most likely $Y$ given $X$.
Define our dataset $\mathcal{D} = \{X^{(n)}, Y^{(n)}\}_{n=1}^N$.
%In this context, $Y$ will not be denoting a single, short sequence but instead an entire corpus to be used for model training. The results of the Baum-Welch algorithm can be derived for many sequences in this corpus and then averaged at each step of iteration.
Let us first define one last helper function $\xi_t(i,j)$ which returns the probability of transitioning from tag $i$ at $t$ to tag $j$ given $X$ and $\theta$. The actual value is now easily derived using the arsenal of other functions we have defined. Consider this as the probability of reaching state $i$ at $t$ given observations $X_1$ to $X_t$, then the probability of transitioning to $j$ and emitting $X_{t+1}$ and finally the probability of reaching state $j$ at $t+1$ given $X_{t+1}$ to $X_T$:
\begin{equation*}
\begin{aligned}
    \xi_t(i,j) \coloneqq & \ptheta(Y_t = i, Y_{t+1} = j \mid X) = \frac{\alpha_t(k)a_{i,j}b_j(X_{t+1})\beta_{t+1}(j)}{\ptheta(X)} \\
    =& \frac{\alpha_t(k)a_{i,j}b_j(X_{t+1})\beta_{t+1}(j)}{\sum_{i \in \mathcal{Y}} \sum_{j \in \mathcal{Y}} \alpha_t(i)a_{i,j}b_j(X_{t+1})\beta_{t+1}(j)}
    \end{aligned} \qquad \begin{array}{lr}
            i,j \in \mathcal{Y} \\
            1 \leq t \leq T-1
        \end{array}
\end{equation*}
Noting that holding the $i$ argument of $\xi$ constant and summing $j$ over all possible values is equivalent to simply the probability of being in tag-state $i$ at position $t$ (or vice versa with $t+1$): $\gamma_t(i) = \sum_{j \in \mathcal{Y}} \xi_t(i,j)$.
With all functions $\alpha, \beta, \gamma, \xi$ now defined we are able to make estimates of how the parameters should be changed at each step of the Baum-Welch algorithm.
Notice that:
\begin{itemize}
    \item $\sum_{t=1}^{T-1} \gamma_t(i)$ is the expected number of times that tag $i$ will be visited (as a non-final node) in $Y$.
    \item $\sum_{t=1}^{T-1} \xi_t(i, j)$ is the expected number of times that tag $i$ will transition to tag $j$ in $Y$.
\end{itemize}
Thanks to these interpretations it becomes much easier to see how the values of $A$, $B$ and $\pi$ should be adjusted.
\begin{gather*}
    \pi_k ' = \gamma_1(i) \\
    a_{i,j}' = \frac{\text{expected \# transitions $i \rightarrow j'$}}{\text{expected \# transitions from $i$}} = \frac{\sum_{t=1}^{T-1} \xi_t(i,j)}{\sum_{t=1}^{T-1} \gamma_t(i)} \\
    b_i(k)' = \frac{\text{expected \# times tag = $i$ and observing $k$}}{\text{expected \# times tag = $i$}} = \frac{\sum_{t=1}^T \gamma_t(i)\ind{X_t = k}}{\sum_{t=1}^{T} \gamma_t(i)}
\end{gather*}

\begin{gather*}
    \pi_k ' = \sum_{n=1}^N \gamma^{(n)}_1(i) \\
    a_{i,j}' = \frac{\sum_{n=1}^N \sum_{t=1}^{T-1} \xi^{(n)}_t(i,j)}{\sum_{n=1}^N \sum_{t=1}^{T-1} \gamma^{(n)}_t(i)} \\
    b_i(k)' = \frac{\sum_{n=1}^N \sum_{t=1}^T \gamma^{(n)}_t(i)\ind{X^{(n)}_t = k}}{\sum_{n=1}^N \sum_{t=1}^{T} \gamma^{(n)}_t(i)}
\end{gather*}

Where $\ind{X_t = k}$ is a boolean indicator function - returning 1 when $X_t=k$ and 0 otherwise.
Since the Baum-Welch algorithm is iterative and convergent the same algorithm can be run again using the new $\theta' = (T', E', \pi')$ and then again, repeatedly until small to no change in values is made.

%\subsection{Limitations}
%Although much stronger than the first model and likely to have a high accuracy rate HMMs are limited in the information that can be incorporated into the predictions they make.
%This may arise as an issue when aiming to incorporate information beyond what is directly specified in our corpus, such as detecting our own features (e.g. suffixes) which can allow our model to make reasonable predictions for words not previously seen and is especially helpful for finely-grained tags such as. distinctions between singular and plural.
%As mentioned earlier the Viterbi algorithm is able to incorporate such additional feature information, even as we move beyond the scope of a hidden Markov model.
%We cannot simply incorporate features such as suffixes as another output layer as they clearly go against the independence of observations assumption ($p(w = \textit{`satisfy'}$ and $p(\text{suffix} = \textit{`-ify'})$ are clearly dependent variables).

\end{document}