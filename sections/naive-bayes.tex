\documentclass[../main.tex]{subfiles}

\begin{document}

We wish to derive a classifier that is able to handle sequences that haven't been encountered in our reference data, since gathering a corpus that contains \textbf{all} sequences and their potential labels at the correct frequencies is impossible (consider that we can construct an infinite number of sequences by joining pre-existing ones).
To do so, we look to construct our likelihood function (and thus $\yhat$) from localised parts of $X$, the individual elements seem to be a suitable start point.
Considering the elements of $X$ in turn will require that we model the interactions between them and the most straightforward way to achieve this is to assume that they are all independent given a fixed value of $Y$. We can impose this condition with the following assumptions:
\begin{assumption}[Independence of Observations] \label{ass:ind-obs}
The probability of each word $X_t$ depends exclusively on the corresponding tag $Y_t$ and is independent of all other elements in either sequence: $p(X \mid Y) = \prod_{t=1}^T p(X_t \mid Y_t)$.
\end{assumption}
\begin{assumption}[Independence of Tags] \label{ass:ind-tags}
The probability of each tag $Y_i$ is independent of all other tags in $Y$: $p(Y) = \prod_{t=1}^T p (Y_i)$.
\end{assumption}

By first applying Bayes' theorem and then our assumptions, we are able to derive $\yhat$. We note as we do so that $\frac{1}{p(X)}$ is constant over $Y$ and so has no bearing on the output of the $\argmax$ function, allowing us to simplify the final expression.
\begin{gather*}
    p(Y \mid X) = \frac{p(X \mid Y)p(Y)}{p(X)} = \frac{1}{p(X)}\prod_{t=1}^T p(X_t \mid Y_t)p(Y_t) = \frac{1}{p(X)}\prod_{t=1}^T p(X_t, Y_t) \\
    \therefore Y^* = \argmax_Y p(Y \mid X) = \argmax_Y \prod_{t=1}^T p(X_t, Y_t)
\end{gather*}
The classifier we have obtained is known as the \textbf{na{\"i}ve Bayes classifier} (here with a single feature, the word). Na{\"i}ve Bayes classifiers are based on the joint probability model
\begin{equation} \label{eqn:nb-model}
    p(Y,X) = p(Y)\prod_{t=1}^T p(X_t \mid Y),
\end{equation}
which emerges from our assumptions.
The connection between our classifier and the na{\"i}ve Bayes model becomes more apparent if one notes that we may have equivalently derived the conditional probability by writing it in terms of the joint probability and applying \cref{eqn:nb-model} before our assumptions.
We can estimate the necessary probabilities $p(X_t,Y_t)$ by taking the relative frequency counts from a tagged reference corpus\footnote{The Penn Treebank is a notable industry-standard example of an annotated corpus. Generally speaking, a reference corpus is simply a very large dataset of text documents.} and can begin to make predictions of the underlying tag sequence.

On inspection of the classifier it becomes apparent that it merely assigns the tag to each word which is most frequently associated with that word.
Such a rudimentary approach is still likely to have a high accuracy rate since many words in the English language have only one appropriate POS tag, however its failure to distinguish between different contexts is a critical flaw and will lead it to fail in many cases of grammatical-ambiguity.

%\subsection{$N$-gram Models}

%We may (correctly) imagine that considerable improvements in accuracy could be made by relaxing the assumption that each word is generated independent of its local context.
%Consider the introductory example sequences `can of fish' and `they can fish'. The na{\"i}ve Bayes classifier we have obtained will label both instances of `can' and `fish' with the same (most common) tags, failing to distinguish between the two contexts.
%We may expect that a model which also incorporated the preceding and/or succeeding words into it's prediction would be more successful.
%This motivates the concept of `$n$-grams' which are sub-sequences of $n$ consecutive words grouped together.
%$N$-grams are used frequently in natural language processing as they provide a better view of semantics by including local context.
%We could incorporate $n$-grams into our model by tweaking our assumptions to consider trigram-tag\footnote{A trigram is an $n$-gram of order 3, that is, a sequence of three words.} pairs rather than word-tag ones, arriving at a new classifier: $Y^* = \argmax_{Y} \prod_{i=1}^n p(Y_{i-1}, Y_i, Y_{i+1} X_{i-1}, X_i, X_{i+1})$ (where the joint probabilities are again estimated by relative frequency).

%This `trigram na{\"i}ve Bayes classifier' gives rise to a number of issues.
%First of all, we are required to define $X_i$ and $Y_i$ when $i$ falls outside of 1 to $n$. We may address this by defining unique `start' and `end' elements, however this will limit the reference data, as we will only be able to inform such probability estimates with references that appear in the exact same position in their own sequences.
%This is indicative of the issues of overfitting and zero-counts which the entire model will suffer from, where certain combinations of words may have so few occurrences in the corpus that they have very high variance (dramatically high/low probability), or have never occurred and thus will have zero probability for all potential tags.

%This best defense against this model bias is  by additive smoothing, a technique which will be used once again with later models.
%Additive smoothing adds a constant term to the numerator which is normalised over all potential tags on the denominator.
%\begin{equation*} \label{eq:bayes-smoothing}
%p(Y_i \mid X_{i-1}, X_{i}) = \frac{\countfunc (Y_i, X_{i-1}, X_i)}{\countfunc (X_{i-1}, X_i)}
%\rightarrow
%\frac{\theta + \countfunc (Y_i, X_{i-1}, X_i)}{\theta |\mathcal{Y}| + \countfunc (X_{i-1}, X_i)}
%\end{equation*}
%It is worth noting that smoothing introduces bias into the model and so the choice of the value of hyper-parameter $\theta$ should be made carefully, ideally with a technique such as cross-validation. 

%It's not statistically rigorous, bizzare independence assumptions are being made... more sensible to replace them entirely and find something more adaptable

\end{document}