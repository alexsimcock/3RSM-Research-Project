\documentclass[../main.tex]{subfiles}

\begin{document}

Any probabilistic classifier must be trained on a dataset in order to achieve reasonable results.
Within this project a dataset of pre-labelled examples $\mathcal{D} = \{\boldX^{(i)},\boldY^{(i)}\}_{i=1}^N$ will be referred to, where $\boldX^{(i)}=(x^{(i)}_1,x^{(i)}_2,\ldots,x^{(i)}_T)$ and $\boldY^{(i)}=(y^{(i)}_1,y^{(i)}_2,\ldots,y^{(i)}_T)$. For notational convenience, the sequences in $\mathcal{D}$ are universally presented as length $T$, in practice $T$ will depend on $i$ and the methods put forth in this project can be easily adapted to reflect this.

The classifier we look to derive must be able to handle sequences that do not appear in our reference data, since gathering a dataset that contains all sequences and their potential labels at the correct frequencies is an impossible task (consider that we can construct an infinite number of sequences by joining pre-existing ones).
As a result, we should look to construct our likelihood function (and thus $\yhat$) from localised parts of $\boldX$ which we can safely assume will appear in the dataset.
Taking the most granular approach, by first considering the individual elements of $\boldX$ seems to be a suitable start point.

Considering each value $x_t$ in turn will require that we model the interactions between them, the most straightforward approach will assume there are no such interactions, that is, $x$ and $x'$ are independent given a fixed value of $\boldY$. We can impose this condition with the following assumptions:
\begin{assumption}[Independence of Observations] \label{ass:ind-obs}
The probability of each word $x_t$ depends exclusively on the corresponding tag $y_t$ and is independent of all other elements in either sequence: $p(\boldX \mid \boldY) = \prod_{t=1}^T p(x_t \mid y_t)$.
\end{assumption}
\begin{assumption}[Independence of Tags] \label{ass:ind-tags}
The probability of each tag $y_t$ is independent of all other tags in $\boldY$: $p(\boldY) = \prod_{t=1}^T p(y_t)$.
\end{assumption}

By first applying Bayes' theorem and then our assumptions, we are able to quickly derive $\yhat$. We note as we do so that $\frac{1}{p(\boldX)}$ is constant over $\boldY$ and so has no bearing on the output of the $\argmax$ function, allowing us to simplify the final expression.
\begin{gather}
    p(\boldY \mid \boldX) = \frac{p(\boldX \mid \boldY)p(\boldY)}{p(\boldX)} = \frac{1}{p(\boldX)}\prod_{t=1}^T p(x_t \mid y_t)p(y_t) = \frac{1}{p(\boldX)}\prod_{t=1}^T p(x_t, y_t), \label{eq:nb-conditional} \\
    \therefore \yhat = \argmax_\boldY p(\boldY \mid \boldX) = \argmax_\boldY \prod_{t=1}^T p(x_t, y_t).
\end{gather}
The classifier we have obtained is known as the \textbf{na{\"i}ve Bayes classifier} (here with a single feature, the word). Na{\"i}ve Bayes classifiers are based on the joint probability model
\begin{equation} \label{eqn:nb-model}
    p(\boldY,\boldX) = p(\boldY)\prod_{t=1}^T p(x_t \mid \boldY),
\end{equation}
which emerges from our assumptions.
The connection between our classifier and the na{\"i}ve Bayes model becomes more apparent if one notes that we may have equivalently derived \cref{eq:nb-conditional} by writing it in terms of the joint probability and applying \cref{eqn:nb-model} before our assumptions.
To complete the classifier, we estimate the necessary probabilities $p(x_t,y_t)$ by taking the relative frequency counts from $\mathcal{D}$ i.e. $p(x_t = k,y_t = i) \hateq [ \sum_{i=1}^N \sum_{t=1}^T \ind{x^{(i)}_t=k}\ind{y^{(i)}_t=i} ] / TN$ where `$\hateq$' indicates estimation rather than equality and $\mathbf{1}$ denotes an indicator function that returns 1 when its condition is true and 0 otherwise.

On inspection of the classifier it becomes apparent that it merely assigns the tag to each word which is most frequently associated with that word.
Such a rudimentary approach is still likely to have a high accuracy rate since many words in the English language have only one appropriate POS tag, but its failure to distinguish between different contexts is a critical flaw and will lead it to fail in many cases of grammatical-ambiguity.

\end{document}